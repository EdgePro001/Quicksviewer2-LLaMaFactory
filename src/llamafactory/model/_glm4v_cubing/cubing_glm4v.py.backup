# cubing_glm4v.py
"""
GLM4V Cubing Module

Based on Quicksviewer's Cubing technique, adapted for GLM4V architecture.
Implements differentiable video adaptive segmentation via Gumbel-Softmax.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


def sample_gumbel(shape, eps=1e-20, dtype=torch.bfloat16):
    """
    Sample Gumbel noise
    
    Args:
        shape: Tensor shape
        eps: Numerical stability constant
        dtype: Data type
    
    Returns:
        Gumbel noise tensor
    """
    U = torch.rand(shape, dtype=dtype, device='cuda')
    return -torch.log(-torch.log(U + eps) + eps)


def gumbel_softmax_sample(logits, temperature, lr_gumbel):
    """
    Gumbel-Softmax sampling
    
    Args:
        logits: [*, n_class] - Unnormalized logits
        temperature: float - Temperature parameter controlling distribution smoothness
        lr_gumbel: float - Weight of Gumbel noise (for annealing)
    
    Returns:
        Sampled softmax probability distribution
    """
    y = logits + sample_gumbel(logits.size(), dtype=logits.dtype) * lr_gumbel
    return F.softmax(y / temperature, dim=-1)


def gumbel_softmax(logits, temperature, topk=1, lr_gumbel=0.1):
    """
    Straight-Through Gumbel-Softmax
    
    Uses hard mask in forward pass and soft gradients in backward pass,
    enabling differentiable discrete sampling.
    
    Args:
        logits: [B, N, 2] - Gate logits, 2 for [non-keyframe, keyframe]
        temperature: float - Gumbel temperature
        topk: int - Select top-k keyframes
        lr_gumbel: float - Gumbel noise weight
    
    Returns:
        y: [B, N] - Soft probabilities (for gradients)
        y_hard: [B, N] - Hard mask (for forward)
    """
    shape = logits.shape
    
    # Gumbel-Softmax sampling
    y = gumbel_softmax_sample(logits, temperature, lr_gumbel)  # [B, N, 2]
    
    # Take second dimension as keyframe probability
    # Note: Consistent with Quicksviewer, dimension 1 represents keyframe
    y = y[:, :, 1]  # [B, N]
    
    # Top-K selection
    _, ind = y.topk(k=topk, dim=-1)  # [B, topk]
    
    # Create hard mask
    y_hard = torch.zeros_like(y)  # [B, N]
    y_hard.scatter_(1, ind, 1)  # Only top-k positions are 1
    
    # Straight-Through: forward uses hard, backward uses soft
    y_hard = (y_hard - y).detach() + y
    
    return y, y_hard


def find_segments(mask):
    """
    Identify continuous segments (cubes) from binary mask
    
    Args:
        mask: [N] - Binary mask, 1 indicates keyframe position
    
    Returns:
        segments: List[(start, end)] - Segment boundaries, half-open [start, end)
    
    Example:
        mask = [1, 0, 0, 1, 0, 1, 0, 0]
        → segments = [(0, 3), (3, 5), (5, 8)]
    """
    segments = []
    pre, cur = 0, 0
    
    while cur < len(mask):
        # End current segment when encountering keyframe or reaching end
        if cur == len(mask) - 1 or (mask[cur + 1] != 0):
            segments.append((pre, cur + 1))
            pre = cur + 1
        cur += 1
    
    return segments


class Glm4vCubingModule(nn.Module):
    """
    GLM4V Cubing Module
    
    Identifies keyframes in video through momentum analysis and segments
    video into multiple cubes. Each cube represents a semantically coherent
    video segment.
    
    Core idea:
        1. Calculate inter-frame momentum: Δ_i = α(F_i - F_{i-1}) + (1-α)Δ_{i-1}
        2. Gate network predicts keyframes
        3. Gumbel-Softmax sampling (differentiable)
        4. Identify cube boundaries
    """
    
    def __init__(self, config):
        """
        Args:
            config: Glm4vConfig object containing:
                - vision_config.hidden_size: Vision feature dimension (1536)
                - text_config.hidden_size: LLM feature dimension (4096)
                - cubing_alpha: Momentum discount factor
                - cubing_use_thumbnail: Whether to generate thumbnail
        """
        super().__init__()
        
        # Configuration parameters
        self.vision_dim = config.vision_config.hidden_size  # 1536
        self.lm_dim = config.text_config.hidden_size  # 4096
        self.alpha = config.cubing_alpha  # 0.8
        self.use_thumbnail = config.cubing_use_thumbnail
        
        # === Frame-level aggregation network ===
        # Purpose: Aggregate 576 patches per frame into 1 vector
        self.agg_frame_fn = nn.Sequential(
            nn.Linear(self.vision_dim, self.vision_dim),
        )
        
        # === Gate network ===
        # Purpose: Predict whether each frame is a keyframe based on momentum features
        # Output: [..., 2], where [:, 0] is non-keyframe prob, [:, 1] is keyframe prob
        self.gate_network = nn.Sequential(
            nn.LayerNorm(self.vision_dim),
            nn.Linear(self.vision_dim, self.vision_dim),
            nn.GELU(),
            nn.Linear(self.vision_dim, 2),
        )
        
        # === Thumbnail projection (optional) ===
        # Purpose: Project weighted average of keyframes to LLM dimension as global video representation
        if self.use_thumbnail:
            self.thumbnail_fn = nn.Sequential(
                nn.Linear(self.vision_dim, self.lm_dim)
            )
    
    def forward(
        self,
        video_features: torch.Tensor,
        fpq: int = 14,
        temperature: float = 0.5,
        lr_gumbel: float = 0.1,
    ):
        """
        Perform Cubing on a single video
        
        Args:
            video_features: [N_temporal_tokens, 576, 1536] - Video frame features
                - N_temporal_tokens: Number of video frames
                - 576: Number of patches per frame (24*24)
                - 1536: Vision feature dimension
            fpq: int - Frames Per Query, target average frames per cube
            temperature: float - Gumbel-Softmax temperature parameter
            lr_gumbel: float - Gumbel noise weight (annealed from 1.0 to 0.001 during training)
        
        Returns:
            dict containing:
                - cube_bounds: List[(start, end)] - List of cube boundaries
                - gate_logits: [N_temporal_tokens-1, 2] - Gate network output (for auxiliary loss)
                - thumbnail: [lm_dim] or None - Global video representation
                - z_hard: [N_temporal_tokens] - Keyframe mask (1 indicates keyframe)
        """
        N_temporal_tokens = video_features.shape[0]
        device = video_features.device
        temporal_patch_size = getattr(
            self.config, 
            'temporal_patch_size', 
            1 
        )
        # print(f"[DEBUG CUBE] video_shapes: {video_features.shape}")
        # print(f"  dtype: {video_features.dtype}")
        # print(f"  LLM output has NaN: {torch.isnan(video_features).any()}")
        # print(f"  LLM output has Inf: {torch.isinf(video_features).any()}")
        
        
        effective_fpq = fpq / temporal_patch_size    
        # print(f"[DEBUG CUBE] effective_fpq: {effective_fpq}")
        
        # ========== Step 1: Calculate momentum  ==========
        # Momentum : Δ_i = α(F_i - F_{i-1}) + (1-α)Δ_{i-1}
        # [N, 576, 1536] → [N-1, 576, 1536]
        vid_feats_momentum = [video_features[1] - video_features[0]]

        for i in range(2, N_temporal_tokens):
            delta = self.alpha * (video_features[i] - video_features[i-1]) + \
                    (1 - self.alpha) * vid_feats_momentum[-1]
            vid_feats_momentum.append(delta)

        vid_feats = torch.stack(vid_feats_momentum, dim=0)  # [N-1, 576, 1536]
        # print(f"[DEBUG CUBE] video_shapes: {vid_feats.shape}")
        # print(f"  dtype: {vid_feats.dtype}")
        # print(f"  LLM output has NaN: {torch.isnan(vid_feats).any()}")
        # print(f"  LLM output has Inf: {torch.isinf(vid_feats).any()}")

        # ========== Step 2: Aggregate momentum features ==========
        # [N-1, 576, 1536] → [N-1, 576, 1536] → [N-1, 1536]
        vid_feats = self.agg_frame_fn(vid_feats)
        # print(f"[DEBUG CUBE] vid_feats after agg_frame_fn: {vid_feats.shape}")
        # print(f"  dtype: {vid_feats.dtype}")
        # print(f"  LLM output has NaN: {torch.isnan(vid_feats).any()}")
        # print(f"  LLM output has Inf: {torch.isinf(vid_feats).any()}")
        vid_feats = vid_feats.mean(dim=1)

        
        # ========== Step 3: Gate network prediction ==========
        # Predict whether each position should be a keyframe based on momentum features
        gate_logits = self.gate_network(vid_feats)  # [N-1, 2]
        
        # ========== Step 4: Gumbel-Softmax sampling ==========
        # Calculate number of keyframes to select
        num_cubes = max(round(N_temporal_tokens / effective_fpq) - 1, 1)
        # Why -1? Because first frame will be forced as keyframe
        
        # Gumbel-Softmax sampling
        z, z_hard = gumbel_softmax(
            gate_logits.unsqueeze(0),  # [1, N-1, 2]
            temperature=temperature,
            topk=num_cubes,
            lr_gumbel=lr_gumbel
        )
        z = z.squeeze(0)  # [N-1]
        z_hard = z_hard.squeeze(0)  # [N-1]
        
        # ========== Step 5: Force first frame as keyframe ==========
        # Ensure video beginning is always a cube start point
        pad_z_hard = torch.ones(1, dtype=z_hard.dtype, device=device)
        z_hard = torch.cat([pad_z_hard, z_hard], dim=0)  # [N]
        
        # ========== Step 6: Identify cube boundaries ==========
        cube_bounds = find_segments(z_hard)
        
        # ========== Step 7: Generate Thumbnail (optional) ==========
        thumbnail = None
        if self.use_thumbnail:
            # Use weighted average of keyframes as global video representation
            weights = z_hard.float() / z_hard.sum()  # Normalized weights
            thumbnail_feat = (vid_feats * weights.unsqueeze(-1)).sum(dim=0)  # [1536]
            thumbnail = self.thumbnail_fn(thumbnail_feat)  # [lm_dim]

        print(f"[DEBUG CUBE] cube_bounds: {cube_bounds}, gate_logits: {gate_logits}, thumbnail: {thumbnail}, z_hard: {z_hard}, z: {z}")
        
        return {
            'cube_bounds': cube_bounds,
            'gate_logits': gate_logits,
            'thumbnail': thumbnail,
            'z_hard': z_hard,
        }
    
    @property
    def config(self):
        """Return configuration dict (for save/load)"""
        return {
            "vision_dim": self.vision_dim,
            "lm_dim": self.lm_dim,
            "alpha": self.alpha,
            "use_thumbnail": self.use_thumbnail,
        }

__all__ = ["Glm4vCubingModule"]