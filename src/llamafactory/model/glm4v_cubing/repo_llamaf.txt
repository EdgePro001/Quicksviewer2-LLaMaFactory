# __init__.py
"""
GLM4V with Cubing Technology for LLaMA-Factory
"""

from .configuration_glm4v_withCube import (
    Glm4vConfig,
    Glm4vTextConfig,
    Glm4vVisionConfig,
)
from .modeling_glm4v_withCube import (
    Glm4vForConditionalGeneration,
    Glm4vModel,
    Glm4vPreTrainedModel,
)
from .processing_glm4v_withCube import Glm4vProcessor
from .image_processing_glm4v import Glm4vCubingImageProcessor

__all__ = [
    "Glm4vConfig",
    "Glm4vTextConfig",
    "Glm4vVisionConfig",
    "Glm4vForConditionalGeneration",
    "Glm4vModel",
    "Glm4vPreTrainedModel",
    "Glm4vProcessor",
    "Glm4vCubingImageProcessor",
]


# configuration_glm4v_withCube.py
from transformers.configuration_utils import PretrainedConfig
from transformers.modeling_rope_utils import rope_config_validation
from transformers import AutoConfig


class Glm4vVisionConfig(PretrainedConfig):
    r"""
    This is the configuration class to store the configuration of a [`Glm4vVisionModel`]. It is used to instantiate an Glm4vVisionModel
    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of
    GLM-4.1V-9B-Thinking [THUDM/GLM-4.1V-9B-Thinking](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking).

    Args:
        hidden_size (`int`, *optional*, defaults to 1536):
            Dimensionality of the encoder layers and the pooler layer.
        depth (`int`, *optional*, defaults to 24):
            Number of layers (depth) in the model.
        attention_bias (`bool`, *optional*, defaults to `False`):
            Whether to add a bias to the queries, keys and values.
        intermediate_size (`int`, *optional*, defaults to 13696):
            Dimensionality of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder.
        hidden_act (`str` or `function`, *optional*, defaults to `"selu"`):
            The non-linear activation function (function or string) in the encoder and pooler. If string, `"gelu"`,
            `"relu"`, `"selu"` and `"gelu_new"` are supported.
        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):
            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.
        attention_dropout (`float`, *optional*, defaults to 0.0):
            Dropout probability for attention weights.
        projection_dropout (`float`, *optional*, defaults to 0.0):
            Dropout probability for the projection layer.
        initializer_range (`float`, *optional*, defaults to 0.02):
            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
        image_size (`int` or `list[int]`, *optional*, defaults to `[336, 336]`):
            The size (resolution) of each image.
        patch_size (`int`, *optional*, defaults to `14`):
            The size (resolution) of each patch.
        num_channels (`int`, *optional*, defaults to 3):
            The number of input channels.
        out_hidden_size (`int`, *optional*, defaults to 4096):
            The output hidden size of the vision model.
        rms_norm_eps (`float`, *optional*, defaults to 1e-05):
            The epsilon used by the rms normalization layers.
        spatial_merge_size (`int`, *optional*, defaults to 2):
            The size used for merging spatial dimensions.
        temporal_patch_size (`int`, *optional*, defaults to 1):
            The size used for patches along the temporal dimension.
    Example:

    ```python
    >>> from transformers import Glm4vVisionConfig, Glm4vVisionModel

    >>> # Initializing a Glm4vVisionConfig GLM-4.1V-9B style configuration
    >>> configuration = Glm4vVisionConfig()

    >>> # Initializing a model (with random weights) from the GLM-4.1V-9B configuration
    >>> model = Glm4vVisionModel(configuration)

    >>> # Accessing the model configuration
    >>> configuration = model.config
    ```"""

    model_type = "glm4v_vision"
    base_config_key = "vision_config"

    def __init__(
        self,
        depth=24,
        hidden_size=1536,
        hidden_act="silu",
        attention_bias=False,
        attention_dropout=0.0,
        num_heads=12,
        in_channels=3,
        image_size=336,
        patch_size=14,
        rms_norm_eps=1e-05,
        spatial_merge_size=2,
        temporal_patch_size=2,
        out_hidden_size=4096,
        intermediate_size=13696,
        initializer_range=0.02,
        **kwargs,
    ):
        super().__init__(**kwargs)

        self.depth = depth
        self.hidden_size = hidden_size
        self.hidden_act = hidden_act
        self.num_heads = num_heads
        self.in_channels = in_channels
        self.image_size = image_size
        self.patch_size = patch_size
        self.spatial_merge_size = spatial_merge_size
        self.temporal_patch_size = temporal_patch_size
        self.out_hidden_size = out_hidden_size
        self.intermediate_size = intermediate_size
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps
        self.attention_bias = attention_bias
        self.attention_dropout = attention_dropout


class Glm4vTextConfig(PretrainedConfig):
    r"""
    This is the configuration class to store the configuration of a [`Glm4vModel`]. It is used to instantiate a
    GLM-4.1V model according to the specified arguments, defining the model architecture. Instantiating a
    configuration with the defaults will yield a similar configuration to that of
    GLM-4.1V-9B-Thinking [THUDM/GLM-4.1V-9B-Thinking](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking).

    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the
    documentation from [`PretrainedConfig`] for more information.

    Args:
        vocab_size (`int`, *optional*, defaults to 151552):
            Vocabulary size of the Glm4v model. Defines the number of different tokens that can be represented by the
            `inputs_ids` passed when calling [`Glm4vModel`]
        hidden_size (`int`, *optional*, defaults to 4096):
            Dimension of the hidden representations.
        intermediate_size (`int`, *optional*, defaults to 13696):
            Dimension of the MLP representations.
        num_hidden_layers (`int`, *optional*, defaults to 40):
            Number of hidden layers in the Transformer encoder.
        num_attention_heads (`int`, *optional*, defaults to 32):
            Number of attention heads for each attention layer in the Transformer encoder.
        num_key_value_heads (`int`, *optional*, defaults to 2):
            This is the number of key_value heads that should be used to implement Grouped Query Attention. If
            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
            by meanpooling all the original heads within that group. For more details checkout [this
            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.
        hidden_act (`str` or `function`, *optional*, defaults to `"silu"`):
            The non-linear activation function (function or string) in the decoder.
        max_position_embeddings (`int`, *optional*, defaults to 32768):
            The maximum sequence length that this model might ever be used with.
        initializer_range (`float`, *optional*, defaults to 0.02):
            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
        rms_norm_eps (`float`, *optional*, defaults to 1e-05):
            The epsilon used by the rms normalization layers.
        use_cache (`bool`, *optional*, defaults to `True`):
            Whether or not the model should return the last key/values attentions (not used by all models). Only
            relevant if `config.is_decoder=True`.
        tie_word_embeddings (`bool`, *optional*, defaults to `False`):
            Whether the model's input and output word embeddings should be tied.
        rope_theta (`float`, *optional*, defaults to 10000.0):
            The base period of the RoPE embeddings.
        attention_dropout (`float`, *optional*, defaults to 0.0):
            The dropout ratio for the attention probabilities.
        rope_scaling (`Dict`, *optional*):
            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type
            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value
            accordingly.
            Expected contents:
                `rope_type` (`str`):
                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',
                    'llama3'], with 'default' being the original RoPE implementation.
                `factor` (`float`, *optional*):
                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In
                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *
                    original maximum pre-trained length.
                `original_max_position_embeddings` (`int`, *optional*):
                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during
                    pretraining.
                `attention_factor` (`float`, *optional*):
                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention
                    computation. If unspecified, it defaults to value recommended by the implementation, using the
                    `factor` field to infer the suggested value.
        image_token_id (`int`, *optional*):
            Token index used as placeholder for image embeddings.
        video_token_id (`int`, *optional*):
            Token index used as placeholder for video embeddings.

    ```python
    >>> from transformers import Glm4vTextModel, Glm4vConfig

    >>> # Initializing a GLM-4.1V style configuration
    >>> configuration = Glm4vConfig()

    >>> # Initializing a model from the GLM-4.1V style configuration
    >>> model = Glm4vTextModel(configuration)

    >>> # Accessing the model configuration
    >>> configuration = model.config
    ```"""

    model_type = "glm4v_text"
    base_config_key = "text_config"
    keys_to_ignore_at_inference = ["past_key_values"]
    # Default tensor parallel plan for base model `Glm4v`
    base_model_tp_plan = {
        "layers.*.self_attn.q_proj": "colwise",
        "layers.*.self_attn.k_proj": "colwise",
        "layers.*.self_attn.v_proj": "colwise",
        "layers.*.self_attn.o_proj": "rowwise",
        "layers.*.mlp.gate_up_proj": "colwise_rep",  # we need to replicate here due to the `chunk` operation
        "layers.*.mlp.down_proj": "rowwise_rep",  # we need to replicate here due to the `chunk` operation
    }
    base_model_pp_plan = {
        "embed_tokens": (["input_ids"], ["inputs_embeds"]),
        "layers": (["hidden_states", "attention_mask"], ["hidden_states"]),
        "norm": (["hidden_states"], ["hidden_states"]),
    }

    def __init__(
        self,
        vocab_size=151552,
        hidden_size=4096,
        intermediate_size=13696,
        num_hidden_layers=40,
        num_attention_heads=32,
        num_key_value_heads=2,
        hidden_act="silu",
        max_position_embeddings=32768,
        initializer_range=0.02,
        rms_norm_eps=1e-05,
        use_cache=True,
        tie_word_embeddings=False,
        rope_theta=10000.0,
        attention_dropout=0.0,
        rope_scaling=None,
        image_token_id=None,
        video_token_id=None,
        **kwargs,
    ):
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads

        # for backward compatibility
        if num_key_value_heads is None:
            num_key_value_heads = num_attention_heads

        self.num_key_value_heads = num_key_value_heads
        self.hidden_act = hidden_act
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps
        self.use_cache = use_cache
        self.rope_theta = rope_theta
        self.attention_dropout = attention_dropout
        if rope_scaling is None:
            rope_scaling = {
                "type": "default",
                "mrope_section": [16, 24, 24]
            }
        self.rope_scaling = rope_scaling
        # Validate the correctness of rotary position embeddings parameters
        # BC: if there is a 'type' field, move it to 'rope_type'.
        if rope_scaling is not None and "type" in self.rope_scaling:
            self.rope_scaling["rope_type"] = self.rope_scaling["type"]
        rope_config_validation(self, ignore_keys={"mrope_section"})

        self.image_token_id = image_token_id
        self.video_token_id = video_token_id

        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)


class Glm4vConfig(PretrainedConfig):
    r"""
    This is the configuration class to store the configuration of a [`Glm4vModel`]. It is used to instantiate a
    GLM-4.1V model according to the specified arguments, defining the model architecture. Instantiating a
    configuration with the defaults will yield a similar configuration to that of
    GLM-4.1V-9B-Thinking [THUDM/GLM-4.1V-9B-Thinking](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking).

    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the
    documentation from [`PretrainedConfig`] for more information.


    Args:
        text_config (`Union[PretrainedConfig, dict]`, *optional*, defaults to `Glm4vTextConfig`):
            The config object or dictionary of the text backbone.
        vision_config (`Union[PretrainedConfig, dict]`,  *optional*, defaults to `Glm4vVisionConfig`):
            The config object or dictionary of the vision backbone.
        image_token_id (`int`, *optional*, defaults to 151343):
            The image token index to encode the image prompt.
        video_token_id (`int`, *optional*, defaults to 151344):
            The video token index to encode the image prompt.
        image_start_token_id (`int`, *optional*, defaults to 151339):
            The image start token index to encode the start of image.
        image_end_token_id (`int`, *optional*, defaults to 151340):
            The image end token index to encode the end of image.
        video_start_token_id (`int`, *optional*, defaults to 151341):
            The video start token index to encode the start of video.
        video_end_token_id (`int`, *optional*, defaults to 151342):
            The video end token index to encode the end of video.

    ```python
    >>> from transformers import Glm4vForConditionalGeneration, Glm4vConfig

    >>> # Initializing a GLM-4.1V style configuration
    >>> configuration = Glm4vConfig()

    >>> # Initializing a model from the GLM-4.1V style configuration
    >>> model = Glm4vForConditionalGeneration(configuration)

    >>> # Accessing the model configuration
    >>> configuration = model.config
    ```"""

    model_type = "glm4v_cubing"
    is_composition = True
    sub_configs = {"vision_config": Glm4vVisionConfig, "text_config": Glm4vTextConfig}
    keys_to_ignore_at_inference = ["past_key_values"]

    def __init__(
        self,
        text_config=None,
        vision_config=None,
        bos_token_id=151331,
        image_token_id=151343,
        video_token_id=151344,
        image_start_token_id=151339,
        image_end_token_id=151340,
        video_start_token_id=151341,
        video_end_token_id=151342,

        # Cubing
        use_cubing=False,
        cubing_alpha=0.8,
        cubing_fpq=7,
        cubing_temperature=0.5,
        cubing_lr_gumbel_start=1.0,
        cubing_lr_gumbel_end=0.001,
        cubing_use_thumbnail=False,

        # Resampler
        resampler_num_queries=64,
        resampler_num_heads=32,
        resampler_max_size=(300, 24, 24),

        initializer_range = 0.02,

        **kwargs,
    ):
        if isinstance(vision_config, dict):
            self.vision_config = self.sub_configs["vision_config"](**vision_config)
        elif vision_config is None:
            self.vision_config = self.sub_configs["vision_config"]()

        if isinstance(text_config, dict):
            self.text_config = self.sub_configs["text_config"](**text_config)
        elif text_config is None:
            self.text_config = self.sub_configs["text_config"](**kwargs)

        self.image_token_id = image_token_id
        self.video_token_id = video_token_id
        self.bos_token_id = bos_token_id
        self.video_start_token_id = video_start_token_id
        self.video_end_token_id = video_end_token_id
        self.image_start_token_id = image_start_token_id
        self.image_end_token_id = image_end_token_id

        # === Cubing ===
        self.use_cubing = use_cubing
        self.cubing_alpha = cubing_alpha
        self.cubing_fpq = cubing_fpq
        self.cubing_temperature = cubing_temperature
        self.cubing_lr_gumbel_start = cubing_lr_gumbel_start
        self.cubing_lr_gumbel_end = cubing_lr_gumbel_end
        self.cubing_use_thumbnail = cubing_use_thumbnail
        
        # === Resampler ===
        self.resampler_num_queries = resampler_num_queries
        self.resampler_num_heads = resampler_num_heads
        self.resampler_max_size = tuple(resampler_max_size) if isinstance(resampler_max_size, list) else resampler_max_size
        
        super().__init__(**kwargs)

        self.initializer_range = initializer_range
        self.architectures = ["Glm4vForConditionalGeneration"]


__all__ = ["Glm4vConfig", "Glm4vTextConfig", "Glm4vVisionConfig"]
AutoConfig.register("glm4v_cubing", Glm4vConfig, exist_ok=True)


# cubing_glm4v.py
"""
GLM4V Cubing Module

Based on Quicksviewer's Cubing technique, adapted for GLM4V architecture.
Implements differentiable video adaptive segmentation via Gumbel-Softmax.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


def sample_gumbel(shape, eps=1e-20, dtype=torch.bfloat16):
    """
    Sample Gumbel noise
    
    Args:
        shape: Tensor shape
        eps: Numerical stability constant
        dtype: Data type
    
    Returns:
        Gumbel noise tensor
    """
    U = torch.rand(shape, dtype=dtype, device='cuda')
    return -torch.log(-torch.log(U + eps) + eps)


def gumbel_softmax_sample(logits, temperature, lr_gumbel):
    """
    Gumbel-Softmax sampling
    
    Args:
        logits: [*, n_class] - Unnormalized logits
        temperature: float - Temperature parameter controlling distribution smoothness
        lr_gumbel: float - Weight of Gumbel noise (for annealing)
    
    Returns:
        Sampled softmax probability distribution
    """
    y = logits + sample_gumbel(logits.size(), dtype=logits.dtype) * lr_gumbel
    return F.softmax(y / temperature, dim=-1)


def gumbel_softmax(logits, temperature, topk=1, lr_gumbel=0.1):
    """
    Straight-Through Gumbel-Softmax
    
    Uses hard mask in forward pass and soft gradients in backward pass,
    enabling differentiable discrete sampling.
    
    Args:
        logits: [B, N, 2] - Gate logits, 2 for [non-keyframe, keyframe]
        temperature: float - Gumbel temperature
        topk: int - Select top-k keyframes
        lr_gumbel: float - Gumbel noise weight
    
    Returns:
        y: [B, N] - Soft probabilities (for gradients)
        y_hard: [B, N] - Hard mask (for forward)
    """
    shape = logits.shape
    
    # Gumbel-Softmax sampling
    y = gumbel_softmax_sample(logits, temperature, lr_gumbel)  # [B, N, 2]
    
    # Take second dimension as keyframe probability
    # Note: Consistent with Quicksviewer, dimension 1 represents keyframe
    y = y[:, :, 1]  # [B, N]
    
    # Top-K selection
    _, ind = y.topk(k=topk, dim=-1)  # [B, topk]
    
    # Create hard mask
    y_hard = torch.zeros_like(y)  # [B, N]
    y_hard.scatter_(1, ind, 1)  # Only top-k positions are 1
    
    # Straight-Through: forward uses hard, backward uses soft
    y_hard = (y_hard - y).detach() + y
    
    return y, y_hard


def find_segments(mask):
    """
    Identify continuous segments (cubes) from binary mask
    
    Args:
        mask: [N] - Binary mask, 1 indicates keyframe position
    
    Returns:
        segments: List[(start, end)] - Segment boundaries, half-open [start, end)
    
    Example:
        mask = [1, 0, 0, 1, 0, 1, 0, 0]
        → segments = [(0, 3), (3, 5), (5, 8)]
    """
    segments = []
    pre, cur = 0, 0
    
    while cur < len(mask):
        # End current segment when encountering keyframe or reaching end
        if cur == len(mask) - 1 or (mask[cur + 1] != 0):
            segments.append((pre, cur + 1))
            pre = cur + 1
        cur += 1
    
    return segments


class Glm4vCubingModule(nn.Module):
    """
    GLM4V Cubing Module
    
    Identifies keyframes in video through momentum analysis and segments
    video into multiple cubes. Each cube represents a semantically coherent
    video segment.
    
    Core idea:
        1. Calculate inter-frame momentum: Δ_i = α(F_i - F_{i-1}) + (1-α)Δ_{i-1}
        2. Gate network predicts keyframes
        3. Gumbel-Softmax sampling (differentiable)
        4. Identify cube boundaries
    """
    
    def __init__(self, config):
        """
        Args:
            config: Glm4vConfig object containing:
                - vision_config.hidden_size: Vision feature dimension (1536)
                - text_config.hidden_size: LLM feature dimension (4096)
                - cubing_alpha: Momentum discount factor
                - cubing_use_thumbnail: Whether to generate thumbnail
        """
        super().__init__()
        
        # Configuration parameters
        self.vision_dim = config.vision_config.hidden_size  # 1536
        self.lm_dim = config.text_config.hidden_size  # 4096
        self.alpha = config.cubing_alpha  # 0.8
        self.use_thumbnail = config.cubing_use_thumbnail
        
        # === Frame-level aggregation network ===
        # Purpose: Aggregate 576 patches per frame into 1 vector
        self.agg_frame_fn = nn.Sequential(
            nn.Linear(self.vision_dim, self.vision_dim),
        )
        
        # === Gate network ===
        # Purpose: Predict whether each frame is a keyframe based on momentum features
        # Output: [..., 2], where [:, 0] is non-keyframe prob, [:, 1] is keyframe prob
        self.gate_network = nn.Sequential(
            nn.LayerNorm(self.vision_dim),
            nn.Linear(self.vision_dim, self.vision_dim),
            nn.GELU(),
            nn.Linear(self.vision_dim, 2),
        )
        
        # === Thumbnail projection (optional) ===
        # Purpose: Project weighted average of keyframes to LLM dimension as global video representation
        if self.use_thumbnail:
            self.thumbnail_fn = nn.Sequential(
                nn.Linear(self.vision_dim, self.lm_dim)
            )
    
    def forward(
        self,
        video_features: torch.Tensor,
        fpq: int = 14,
        temperature: float = 0.5,
        lr_gumbel: float = 0.1,
    ):
        """
        Perform Cubing on a single video
        
        Args:
            video_features: [N_temporal_tokens, 576, 1536] - Video frame features
                - N_temporal_tokens: Number of video frames
                - 576: Number of patches per frame (24*24)
                - 1536: Vision feature dimension
            fpq: int - Frames Per Query, target average frames per cube
            temperature: float - Gumbel-Softmax temperature parameter
            lr_gumbel: float - Gumbel noise weight (annealed from 1.0 to 0.001 during training)
        
        Returns:
            dict containing:
                - cube_bounds: List[(start, end)] - List of cube boundaries
                - gate_logits: [N_temporal_tokens-1, 2] - Gate network output (for auxiliary loss)
                - thumbnail: [lm_dim] or None - Global video representation
                - z_hard: [N_temporal_tokens] - Keyframe mask (1 indicates keyframe)
        """
        N_temporal_tokens = video_features.shape[0]
        device = video_features.device
        temporal_patch_size = getattr(
            self.config, 
            'temporal_patch_size', 
            1 
        )
        print(f"[DEBUG CUBE] video_shapes: {video_features.shape}")
        print(f"  dtype: {video_features.dtype}")
        print(f"  LLM output has NaN: {torch.isnan(video_features).any()}")
        print(f"  LLM output has Inf: {torch.isinf(video_features).any()}")
        
        
        effective_fpq = fpq / temporal_patch_size    
        print(f"[DEBUG CUBE] effective_fpq: {effective_fpq}")
        
        # ========== Step 1: Calculate momentum  ==========
        # Momentum : Δ_i = α(F_i - F_{i-1}) + (1-α)Δ_{i-1}
        # [N, 576, 1536] → [N-1, 576, 1536]
        vid_feats_momentum = [video_features[1] - video_features[0]]

        for i in range(2, N_temporal_tokens):
            delta = self.alpha * (video_features[i] - video_features[i-1]) + \
                    (1 - self.alpha) * vid_feats_momentum[-1]
            vid_feats_momentum.append(delta)

        vid_feats = torch.stack(vid_feats_momentum, dim=0)  # [N-1, 576, 1536]
        print(f"[DEBUG CUBE] video_shapes: {vid_feats.shape}")
        print(f"  dtype: {vid_feats.dtype}")
        print(f"  LLM output has NaN: {torch.isnan(vid_feats).any()}")
        print(f"  LLM output has Inf: {torch.isinf(vid_feats).any()}")

        # ========== Step 2: Aggregate momentum features ==========
        # [N-1, 576, 1536] → [N-1, 576, 1536] → [N-1, 1536]
        vid_feats = self.agg_frame_fn(vid_feats)
        print(f"[DEBUG CUBE] vid_feats after agg_frame_fn: {vid_feats.shape}")
        print(f"  dtype: {vid_feats.dtype}")
        print(f"  LLM output has NaN: {torch.isnan(vid_feats).any()}")
        print(f"  LLM output has Inf: {torch.isinf(vid_feats).any()}")
        vid_feats = vid_feats.mean(dim=1)

        
        # ========== Step 3: Gate network prediction ==========
        # Predict whether each position should be a keyframe based on momentum features
        gate_logits = self.gate_network(vid_feats)  # [N-1, 2]
        
        # ========== Step 4: Gumbel-Softmax sampling ==========
        # Calculate number of keyframes to select
        num_cubes = max(round(N_temporal_tokens / effective_fpq) - 1, 1)
        # Why -1? Because first frame will be forced as keyframe
        
        # Gumbel-Softmax sampling
        z, z_hard = gumbel_softmax(
            gate_logits.unsqueeze(0),  # [1, N-1, 2]
            temperature=temperature,
            topk=num_cubes,
            lr_gumbel=lr_gumbel
        )
        z = z.squeeze(0)  # [N-1]
        z_hard = z_hard.squeeze(0)  # [N-1]
        
        # ========== Step 5: Force first frame as keyframe ==========
        # Ensure video beginning is always a cube start point
        pad_z_hard = torch.ones(1, dtype=z_hard.dtype, device=device)
        z_hard = torch.cat([pad_z_hard, z_hard], dim=0)  # [N]
        
        # ========== Step 6: Identify cube boundaries ==========
        cube_bounds = find_segments(z_hard)
        
        # ========== Step 7: Generate Thumbnail (optional) ==========
        thumbnail = None
        if self.use_thumbnail:
            # Use weighted average of keyframes as global video representation
            weights = z_hard.float() / z_hard.sum()  # Normalized weights
            thumbnail_feat = (vid_feats * weights.unsqueeze(-1)).sum(dim=0)  # [1536]
            thumbnail = self.thumbnail_fn(thumbnail_feat)  # [lm_dim]

        print(f"[DEBUG CUBE] cube_bounds: {cube_bounds}, gate_logits: {gate_logits}, thumbnail: {thumbnail}, z_hard: {z_hard}, z: {z}")
        
        return {
            'cube_bounds': cube_bounds,
            'gate_logits': gate_logits,
            'thumbnail': thumbnail,
            'z_hard': z_hard,
        }
    
    @property
    def config(self):
        """Return configuration dict (for save/load)"""
        return {
            "vision_dim": self.vision_dim,
            "lm_dim": self.lm_dim,
            "alpha": self.alpha,
            "use_thumbnail": self.use_thumbnail,
        }

__all__ = ["Glm4vCubingModule"]



# image_processing_glm4v.py
"""Image processor class for GLM-4.1V."""

import math
from typing import Optional, Union

import numpy as np

from transformers.image_processing_utils import BaseImageProcessor, BatchFeature
from transformers.image_transforms import (
    convert_to_rgb,
    resize,
    to_channel_dimension_format,
)
from transformers.image_utils import (
    OPENAI_CLIP_MEAN,
    OPENAI_CLIP_STD,
    ChannelDimension,
    ImageInput,
    PILImageResampling,
    get_image_size,
    infer_channel_dimension_format,
    is_scaled_image,
    make_flat_list_of_images,
    to_numpy_array,
    valid_images,
    validate_preprocess_arguments,
)
from transformers.processing_utils import ImagesKwargs
from transformers.utils import TensorType, logging
from transformers.video_utils import VideoInput


logger = logging.get_logger(__name__)


class Glm4vImageProcessorKwargs(ImagesKwargs, total=False):
    """
    patch_size (`int`, *optional*, defaults to 14):
        The spatial patch size of the vision encoder.
    temporal_patch_size (`int`, *optional*, defaults to 2):
        The temporal patch size of the vision encoder.
    merge_size (`int`, *optional*, defaults to 2):
        The merge size of the vision encoder to llm encoder.
    """

    patch_size: int
    temporal_patch_size: int
    merge_size: int


def smart_resize(
    num_frames: int,
    height: int,
    width: int,
    temporal_factor: int = 2,
    factor: int = 28,
    min_pixels: int = 112 * 112,
    max_pixels: int = 14 * 14 * 2 * 2 * 2 * 6144,
):
    if num_frames < temporal_factor:
        raise ValueError(f"t:{num_frames} must be larger than temporal_factor:{temporal_factor}")
    if height < factor or width < factor:
        raise ValueError(f"height:{height} or width:{width} must be larger than factor:{factor}")
    elif max(height, width) / min(height, width) > 200:
        raise ValueError(
            f"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}"
        )
    h_bar = round(height / factor) * factor
    w_bar = round(width / factor) * factor
    t_bar = round(num_frames / temporal_factor) * temporal_factor

    if t_bar * h_bar * w_bar > max_pixels:
        beta = math.sqrt((num_frames * height * width) / max_pixels)
        h_bar = max(factor, math.floor(height / beta / factor) * factor)
        w_bar = max(factor, math.floor(width / beta / factor) * factor)
    elif t_bar * h_bar * w_bar < min_pixels:
        beta = math.sqrt(min_pixels / (num_frames * height * width))
        h_bar = math.ceil(height * beta / factor) * factor
        w_bar = math.ceil(width * beta / factor) * factor

    return h_bar, w_bar


class Glm4vCubingImageProcessor(BaseImageProcessor):
    r"""
    Constructs a GLM-4V image processor that dynamically resizes images based on the original images.

    Args:
        do_resize (`bool`, *optional*, defaults to `True`):
            Whether to resize the image's (height, width) dimensions.
        size (`Dict[str, int]` *optional*, defaults to `{"shortest_edge": 112 * 112, "longest_edge": 28 * 28 * 15000}`):
            Size of the image's `(height, width)` dimensions after resizing. Can be overridden by the `size` parameter
            in the `preprocess` method. Available options are:
                - `{"height": int, "width": int}`: The image will be resized to the exact size `(height, width)`.
                    Do NOT keep the aspect ratio.
                - `{"shortest_edge": int, "longest_edge": int}`: The image will be resized to a maximum size respecting
                    the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge
                    less or equal to `longest_edge`.
                - `{"max_height": int, "max_width": int}`: The image will be resized to the maximum size respecting the
                    aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to
                    `max_width`.
        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):
            Resampling filter to use when resizing the image.
        do_rescale (`bool`, *optional*, defaults to `True`):
            Whether to rescale the image by the specified scale `rescale_factor`.
        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):
            Scale factor to use if rescaling the image.
        do_normalize (`bool`, *optional*, defaults to `True`):
            Whether to normalize the image.
        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):
            Mean to use if normalizing the image. This is a float or list of floats for each channel in the image.
        image_std (`float` or `List[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):
            Standard deviation to use if normalizing the image. This is a float or list of floats for each channel in the image.
        do_convert_rgb (`bool`, *optional*, defaults to `True`):
            Whether to convert the image to RGB.
        patch_size (`int`, *optional*, defaults to 14):
            The spatial patch size of the vision encoder.
        temporal_patch_size (`int`, *optional*, defaults to 2):
            The temporal patch size of the vision encoder.
        merge_size (`int`, *optional*, defaults to 2):
            The merge size of the vision encoder to llm encoder.
    """

    model_input_names = ["pixel_values", "image_grid_thw"]
    valid_kwargs = Glm4vImageProcessorKwargs

    def __init__(
        self,
        do_resize: bool = True,
        size: Optional[dict[str, int]] = None,
        resample: PILImageResampling = PILImageResampling.BICUBIC,
        do_rescale: bool = True,
        rescale_factor: Union[int, float] = 1 / 255,
        do_normalize: bool = True,
        image_mean: Optional[Union[float, list[float]]] = None,
        image_std: Optional[Union[float, list[float]]] = None,
        do_convert_rgb: bool = True,
        patch_size: int = 14,
        temporal_patch_size: int = 2,
        merge_size: int = 2,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        if size is not None and ("shortest_edge" not in size or "longest_edge" not in size):
            raise ValueError("size must contain 'shortest_edge' and 'longest_edge' keys.")
        elif size is None:
            size = {"shortest_edge": 112 * 112, "longest_edge": 28 * 28 * 15000}
        self.size = size

        self.do_resize = do_resize
        self.resample = resample
        self.do_rescale = do_rescale
        self.rescale_factor = rescale_factor
        self.do_normalize = do_normalize
        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN
        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD

        self.patch_size = patch_size
        self.temporal_patch_size = temporal_patch_size
        self.merge_size = merge_size
        self.do_convert_rgb = do_convert_rgb

    def _preprocess(
        self,
        images: Union[ImageInput, VideoInput],
        do_resize: Optional[bool] = None,
        size: Optional[dict[str, int]] = None,
        resample: Optional[PILImageResampling] = None,
        do_rescale: Optional[bool] = None,
        rescale_factor: Optional[float] = None,
        do_normalize: Optional[bool] = None,
        image_mean: Optional[Union[float, list[float]]] = None,
        image_std: Optional[Union[float, list[float]]] = None,
        patch_size: Optional[int] = None,
        temporal_patch_size: Optional[int] = None,
        merge_size: Optional[int] = None,
        do_convert_rgb: Optional[bool] = None,
        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,
        input_data_format: Optional[Union[str, ChannelDimension]] = None,
    ):
        """
        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.

        Args:
            images (`ImageInput`):
                Image or batch of images to preprocess. Expects pixel values ranging from 0 to 255. If pixel values range from 0 to 1, set `do_rescale=False`.
            vision_info (`List[Dict]`, *optional*):
                Optional list of dictionaries containing additional information about vision inputs.
            do_resize (`bool`, *optional*, defaults to `self.do_resize`):
                Whether to resize the image.
            size (`Dict[str, int]`, *optional*, defaults to `self.size`):
                Size of the image after resizing. `shortest_edge` and `longest_edge` keys must be present.
            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):
                Resampling filter to use if resizing the image. This can be one of the `PILImageResampling` enums.
            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):
                Whether to rescale the image.
            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):
                Scale factor to use if rescaling the image.
            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):
                Whether to normalize the image.
            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):
                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.
            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):
                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.
            patch_size (`int`, *optional*, defaults to `self.patch_size`):
                The spatial patch size of the vision encoder.
            temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):
                The temporal patch size of the vision encoder.
            merge_size (`int`, *optional*, defaults to `self.merge_size`):
                The merge size of the vision encoder to llm encoder.
            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):
                Whether to convert the image to RGB.
            data_format (`ChannelDimension`, *optional*, defaults to `ChannelDimension.FIRST`):
                The channel dimension format for the output image. Can be one of:
                - `"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.
                - `"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.
                - Unset: Use the channel dimension format of the input image.
            input_data_format (`ChannelDimension` or `str`, *optional*):
                The channel dimension format for the input image. Can be one of:
                - `"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.
                - `"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.
                - `"none"` or `ChannelDimension.NONE`: image in (height, width) format.   - `"none"` or `ChannelDimension.NONE`: image in (height, width) format.
        """
        images = make_flat_list_of_images(images)

        if do_convert_rgb:
            images = [convert_to_rgb(image) for image in images]

        # All transformations expect numpy arrays.
        images = [to_numpy_array(image) for image in images]

        if do_rescale and is_scaled_image(images[0]):
            logger.warning_once(
                "It looks like you are trying to rescale already rescaled images. If the input"
                " images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again."
            )
        if input_data_format is None:
            # We assume that all images have the same channel dimension format.
            input_data_format = infer_channel_dimension_format(images[0])

        height, width = get_image_size(images[0], channel_dim=input_data_format)
        resized_height, resized_width = height, width
        processed_images = []
        for image in images:
            if do_resize:
                resized_height, resized_width = smart_resize(
                    num_frames=temporal_patch_size,
                    height=height,
                    width=width,
                    temporal_factor=temporal_patch_size,
                    factor=patch_size * merge_size,
                    min_pixels=size["shortest_edge"],
                    max_pixels=size["longest_edge"],
                )
                image = resize(
                    image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format
                )

            if do_rescale:
                image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)

            if do_normalize:
                image = self.normalize(
                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format
                )

            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)
            processed_images.append(image)

        patches = np.array(processed_images)
        if data_format == ChannelDimension.LAST:
            patches = patches.transpose(0, 3, 1, 2)
        if patches.shape[0] % temporal_patch_size != 0:
            repeats = np.repeat(
                patches[-1][np.newaxis], temporal_patch_size - (patches.shape[0] % temporal_patch_size), axis=0
            )
            patches = np.concatenate([patches, repeats], axis=0)
        channel = patches.shape[1]
        grid_t = patches.shape[0] // temporal_patch_size
        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size
        patches = patches.reshape(
            grid_t,
            temporal_patch_size,
            channel,
            grid_h // merge_size,
            merge_size,
            patch_size,
            grid_w // merge_size,
            merge_size,
            patch_size,
        )
        patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)
        flatten_patches = patches.reshape(
            grid_t * grid_h * grid_w, channel * temporal_patch_size * patch_size * patch_size
        )

        return flatten_patches, (grid_t, grid_h, grid_w)

    def preprocess(
        self,
        images: ImageInput,
        videos: Optional[VideoInput] = None,
        do_resize: Optional[bool] = None,
        size: Optional[dict[str, int]] = None,
        resample: Optional[PILImageResampling] = None,
        do_rescale: Optional[bool] = None,
        rescale_factor: Optional[float] = None,
        do_normalize: Optional[bool] = None,
        image_mean: Optional[Union[float, list[float]]] = None,
        image_std: Optional[Union[float, list[float]]] = None,
        patch_size: Optional[int] = None,
        temporal_patch_size: Optional[int] = None,
        merge_size: Optional[int] = None,
        do_convert_rgb: Optional[bool] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,
        input_data_format: Optional[Union[str, ChannelDimension]] = None,
    ):
        """
        Args:
            images (`ImageInput`):
                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
                passing in images with pixel values between 0 and 1, set `do_rescale=False`.
            videos (`VideoInput`):
                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If
                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.
            do_resize (`bool`, *optional*, defaults to `self.do_resize`):
                Whether to resize the image.
            size (`Dict[str, int]`, *optional*, defaults to `self.size`):
                Size of the image after resizing. Shortest edge of the image is resized to size["shortest_edge"], with
                the longest edge resized to keep the input aspect ratio.
            resample (`int`, *optional*, defaults to `self.resample`):
                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only
                has an effect if `do_resize` is set to `True`.
            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):
                Whether to rescale the image.
            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):
                Rescale factor to rescale the image by if `do_rescale` is set to `True`.
            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):
                Whether to normalize the image.
            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):
                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.
            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):
                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to
                `True`.
                The max pixels of the image to resize the image.
            patch_size (`int`, *optional*, defaults to `self.patch_size`):
                The spatial patch size of the vision encoder.
            temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):
                The temporal patch size of the vision encoder.
            merge_size (`int`, *optional*, defaults to `self.merge_size`):
                The merge size of the vision encoder to llm encoder.
            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):
                Whether to convert the image to RGB.
            return_tensors (`str` or `TensorType`, *optional*):
                The type of tensors to return. Can be one of:
                - Unset: Return a list of `np.ndarray`.
                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.
                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.
            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):
                The channel dimension format for the output image. Can be one of:
                - `"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.
                - `"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.
                - Unset: Use the channel dimension format of the input image.
            input_data_format (`ChannelDimension` or `str`, *optional*):
                The channel dimension format for the input image. If unset, the channel dimension format is inferred
                from the input image. Can be one of:
                - `"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.
                - `"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.
                - `"none"` or `ChannelDimension.NONE`: image in (height, width) format.

        """
        # Try to use config values if set, otherwise fallback to global defaults
        size = size if size is not None else self.size
        if size is not None and ("shortest_edge" not in size or "longest_edge" not in size):
            raise ValueError("size must contain 'shortest_edge' and 'longest_edge' keys.")
        elif size is None:
            size = {"shortest_edge": 112 * 112, "longest_edge": 28 * 28 * 15000}

        do_resize = do_resize if do_resize is not None else self.do_resize
        resample = resample if resample is not None else self.resample
        do_rescale = do_rescale if do_rescale is not None else self.do_rescale
        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor
        do_normalize = do_normalize if do_normalize is not None else self.do_normalize
        image_mean = image_mean if image_mean is not None else self.image_mean
        image_std = image_std if image_std is not None else self.image_std
        patch_size = patch_size if patch_size is not None else self.patch_size
        temporal_patch_size = temporal_patch_size if temporal_patch_size is not None else self.temporal_patch_size
        merge_size = merge_size if merge_size is not None else self.merge_size
        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb

        if images is not None:
            images = self.fetch_images(images)
            images = make_flat_list_of_images(images)

        if images is not None and not valid_images(images):
            raise ValueError("Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor")

        validate_preprocess_arguments(
            rescale_factor=rescale_factor,
            do_normalize=do_normalize,
            image_mean=image_mean,
            image_std=image_std,
            do_resize=do_resize,
            size=size,
            resample=resample,
        )

        data = {}
        if images is not None:
            pixel_values, vision_grid_thws = [], []
            for image in images:
                patches, image_grid_thw = self._preprocess(
                    image,
                    do_resize=do_resize,
                    size=size,
                    resample=resample,
                    do_rescale=do_rescale,
                    rescale_factor=rescale_factor,
                    do_normalize=do_normalize,
                    image_mean=image_mean,
                    image_std=image_std,
                    patch_size=patch_size,
                    temporal_patch_size=temporal_patch_size,
                    merge_size=merge_size,
                    data_format=data_format,
                    do_convert_rgb=do_convert_rgb,
                    input_data_format=input_data_format,
                )
                pixel_values.extend(patches)
                vision_grid_thws.append(image_grid_thw)
            pixel_values = np.array(pixel_values)
            vision_grid_thws = np.array(vision_grid_thws)
            data.update({"pixel_values": pixel_values, "image_grid_thw": vision_grid_thws})

        return BatchFeature(data=data, tensor_type=return_tensors)

    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):
        """
        A utility that returns number of image patches for a given image size.

        Args:
            height (`int`):
                Height of the input image.
            width (`int`):
                Width of the input image.
            images_kwargs (`dict`, *optional*)
                Any kwargs to override defaults of the image processor.
        Returns:
            `int`: Number of image patches per image.
        """
        patch_size = images_kwargs.get("patch_size", self.patch_size)
        merge_size = images_kwargs.get("merge_size", self.merge_size)
        size = images_kwargs.get("size", {"shortest_edge": 112 * 112, "longest_edge": 28 * 28 * 15000})

        factor = patch_size * merge_size
        resized_height, resized_width = smart_resize(
            num_frames=self.temporal_patch_size,
            height=height,
            width=width,
            factor=factor,
            min_pixels=size["shortest_edge"],
            max_pixels=size["longest_edge"],
            temporal_factor=self.temporal_patch_size,
        )
        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size
        return grid_h * grid_w


__all__ = ["Glm4vCubingImageProcessor"]



# coding = utf-8
# modeling_glm4v_withCube.py

import itertools
from collections.abc import Callable
from dataclasses import dataclass
from typing import Any, Optional, Union, List, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import LayerNorm
from torch.nn.init import trunc_normal_

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache
from transformers.generation import GenerationMixin
from transformers.integrations import use_kernel_forward_from_hub
from transformers.masking_utils import create_causal_mask
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_outputs import BaseModelOutputWithPast, ModelOutput
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling
from transformers.utils.generic import check_model_inputs
from .configuration_glm4v_withCube import Glm4vConfig, Glm4vTextConfig, Glm4vVisionConfig
from .cubing_glm4v import Glm4vCubingModule
from .resampler_glm4v import Glm4vResampler

from transformers import AutoModel, AutoModelForCausalLM


@dataclass
class Glm4vCubingVideoMetadata:
    original_grid_thw: torch.Tensor  # [[20, 24, 24]] - 视频级别
    actual_num_tokens: Optional[int] = None  # 192 - 实际生成的 tokens
    mode: str = "native"  # "native" 或 "cubing"
    
    # Cubing 
    cube_bounds: Optional[List[List[Tuple[int, int]]]] = None  # [[(0, 7), (7, 14), ...]]
    temporal_patch_size: int = 2
    
    def to_flattened(self) -> torch.Tensor:
        """转换成 temporal token 级别格式"""
        result = []
        for t, h, w in self.original_grid_thw:
            num_temporal_tokens = t.item() // self.temporal_patch_size
            # 关键：每个 temporal token 的 T 维度
            repeated = torch.tensor(
                [[self.temporal_patch_size, h.item(), w.item()]] * num_temporal_tokens,
                device=self.original_grid_thw.device,
                dtype=self.original_grid_thw.dtype
            )
            result.append(repeated)
        
        return torch.cat(result, dim=0)


@use_kernel_forward_from_hub("RMSNorm")
class Glm4vRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        Glm4vRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Glm4VisionMlp(nn.Module):
    def __init__(self, config, bias: bool = False):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.out_hidden_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=bias)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=bias)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=bias)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, hidden_state):
        return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))


class Glm4vVisionPatchEmbed(nn.Module):
    def __init__(self, config: Glm4vVisionConfig) -> None:
        super().__init__()
        self.patch_size = config.patch_size
        self.temporal_patch_size = config.temporal_patch_size
        self.in_channels = config.in_channels
        self.embed_dim = config.hidden_size

        kernel_size = [self.temporal_patch_size, self.patch_size, self.patch_size]
        self.proj = nn.Conv3d(self.in_channels, self.embed_dim, kernel_size=kernel_size, stride=kernel_size)
        self.use_3d = True

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        target_dtype = self.proj.weight.dtype

        if self.use_3d:
            hidden_states = hidden_states.view(
                -1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size
            )
            hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)
        else:
            N, C, H, W = hidden_states.shape
        
            # [N, C, H, W] -> [N, C, H//patch_size, patch_size, W//patch_size, patch_size]
            h_patches = H // self.patch_size
            w_patches = W // self.patch_size
            
            hidden_states = hidden_states.reshape(
                N, C,
                h_patches, self.patch_size,
                w_patches, self.patch_size
            )
            # [N, C, h_patches, patch_size, w_patches, patch_size]
            # -> [N, C, h_patches, w_patches, patch_size, patch_size]
            hidden_states = hidden_states.permute(0, 1, 2, 4, 3, 5)
            # -> [N*h_patches*w_patches, C, patch_size, patch_size]
            hidden_states = hidden_states.reshape(-1, C, self.patch_size, self.patch_size)
            
            hidden_states = self.proj(hidden_states.to(dtype=target_dtype))  # [N*h*w, embed_dim, 1, 1]
            hidden_states = hidden_states.view(-1, self.embed_dim)  # [N*h*w, embed_dim]

        return hidden_states
            


class Glm4vVisionRotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, dim: int, theta: float = 10000.0) -> None:
        super().__init__()
        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)

    def forward(self, seqlen: int) -> torch.Tensor:
        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
        freqs = torch.outer(seq, self.inv_freq)
        return freqs


class Glm4vVisionPatchMerger(nn.Module):
    def __init__(self, dim: int, context_dim: int, hidden_act: str, bias: bool = False) -> None:
        super().__init__()
        self.proj = nn.Linear(dim, dim, bias=bias)
        self.post_projection_norm = LayerNorm(dim)
        self.gate_proj = nn.Linear(dim, context_dim, bias=bias)
        self.up_proj = nn.Linear(dim, context_dim, bias=bias)
        self.down_proj = nn.Linear(context_dim, dim, bias=bias)
        self.act1 = nn.GELU()
        self.act_fn = ACT2FN[hidden_act]

    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:
        hidden_state = self.proj(hidden_state)
        hidden_state = self.act1(self.post_projection_norm(hidden_state))
        return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))


class Glm4vVisionEmbeddings(nn.Module):
    def __init__(self, config: Glm4vVisionConfig):
        super().__init__()
        self.config = config
        self.embed_dim = config.hidden_size
        self.image_size = config.image_size
        self.patch_size = config.patch_size

        self.num_patches = (self.image_size // self.patch_size) ** 2
        self.num_positions = self.num_patches
        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)
        self.register_buffer("position_ids", torch.arange(self.num_positions).expand((1, -1)), persistent=False)

    def forward(self, embeddings, lengths, image_shapes, h_coords, w_coords) -> torch.Tensor:
        """
        Forward pass with integrated position encoding adaptation using 2D interpolation.

        Args:
            embeddings: Input embeddings tensor
            lengths (torch.Tensor): Sequence lengths for each image in the batch.
            image_shapes (torch.Tensor): Tensor of shape [batch_size, 3] representing the image shapes (t, h, w).
            h_coords (torch.Tensor): Tensor of shape [total_seq] representing the h coordinate for each patch.
            w_coords (torch.Tensor): Tensor of shape [total_seq] representing the w coordinate for each patch.

        Returns:
            torch.Tensor: Embeddings with adapted position encoding added.
        """
        
        pos_embed_weight = self.position_embedding.weight
        hidden_size = pos_embed_weight.shape[1]
        total_seq = h_coords.shape[0]
        device = pos_embed_weight.device

        h_coords, w_coords = h_coords.to(device), w_coords.to(device)

        # Handle empty sequence case
        if total_seq == 0:
            adapted_pos_embed = torch.empty(0, hidden_size, device=device, dtype=pos_embed_weight.dtype)
        else:
            # Convert inputs to tensors if needed
            if isinstance(lengths, list):
                lengths = torch.tensor(lengths, device=device, dtype=torch.long)
            if not isinstance(image_shapes, torch.Tensor):
                image_shapes = torch.tensor(image_shapes, device=device, dtype=torch.long)

            # Prepare 2D position embedding
            orig_size_sq = pos_embed_weight.shape[0]
            orig_size = int(orig_size_sq**0.5)
            pos_embed_2d = (
                pos_embed_weight.view(orig_size, orig_size, hidden_size)
                .permute(2, 0, 1)
                .unsqueeze(0)
                .to(device=device, dtype=torch.float32)
            )

            # Calculate target dimensions for each patch
            if len(lengths) > image_shapes.shape[0]:
                h_val = image_shapes[0, 1]
                w_val = image_shapes[0, 2]
                target_h = torch.cat([h_val.repeat(lengths[i]) for i in range(len(lengths))]).to(
                    device=device, dtype=torch.float32
                )
                target_w = torch.cat([w_val.repeat(lengths[i]) for i in range(len(lengths))]).to(
                    device=device, dtype=torch.float32
                )
            else:
                target_h = torch.cat([image_shapes[i, 1].repeat(lengths[i]) for i in range(len(lengths))]).to(
                    device=device, dtype=torch.float32
                )
                target_w = torch.cat([image_shapes[i, 2].repeat(lengths[i]) for i in range(len(lengths))]).to(
                    device=device, dtype=torch.float32
                )

            # Normalize coordinates to [-1, 1] range for grid_sample
            h_coords = h_coords.to(device=device, dtype=torch.float32)
            w_coords = w_coords.to(device=device, dtype=torch.float32)
            norm_w = ((w_coords + 0.5) / target_w) * 2 - 1
            norm_h = ((h_coords + 0.5) / target_h) * 2 - 1

            # Create sampling grid
            grid = torch.stack((norm_w, norm_h), dim=-1).unsqueeze(0).unsqueeze(2)

            # Perform bicubic interpolation
            interpolated_embed_fp32 = F.grid_sample(
                pos_embed_2d, grid, mode="bicubic", align_corners=False, padding_mode="border"
            )

            # Reshape and convert back to original dtype
            adapted_pos_embed_fp32 = interpolated_embed_fp32.squeeze(0).squeeze(-1).permute(1, 0)
            adapted_pos_embed = adapted_pos_embed_fp32.to(pos_embed_weight.dtype).to(embeddings.device)

        if embeddings.shape[0] != adapted_pos_embed.shape[0]:
            num_repeats = embeddings.shape[0] // adapted_pos_embed.shape[0]
            
            # 确保是整倍数，否则会出错
            if embeddings.shape[0] % adapted_pos_embed.shape[0] != 0:
                raise ValueError(
                    f"Embeddings dimension ({embeddings.shape[0]}) is not a multiple of "
                    f"position embeddings dimension ({adapted_pos_embed.shape[0]})"
                )
                
            adapted_pos_embed = adapted_pos_embed.repeat(num_repeats, 1)

        # Add adapted position encoding to embeddings
        embeddings = embeddings + adapted_pos_embed

        return embeddings


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb_vision(
    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor
) -> tuple[torch.Tensor, torch.Tensor]:
    if q.shape[0] != cos.shape[0]:
        num_repeats = q.shape[0] // cos.shape[0]
        
        # 确保是整倍数
        if q.shape[0] % cos.shape[0] != 0:
            raise ValueError(
                f"Query dimension ({q.shape[0]}) is not a multiple of "
                f"Cos/Sin dimension ({cos.shape[0]})"
            )
        
        # cos/sin 原始shape是 [10752, head_dim]
        # 将它们复制以匹配 q 的 batch 维度
        cos = cos.repeat(num_repeats, 1) # new shape [21504, head_dim]
        sin = sin.repeat(num_repeats, 1) # new shape [21504, head_dim]

    orig_q_dtype = q.dtype
    orig_k_dtype = k.dtype
    q, k = q.float(), k.float()
    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    q_embed = q_embed.to(orig_q_dtype)
    k_embed = k_embed.to(orig_k_dtype)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class Glm4vVisionAttention(nn.Module):
    def __init__(self, config: Glm4vVisionConfig) -> None:
        super().__init__()
        self.dim = config.hidden_size
        self.num_heads = config.num_heads
        self.head_dim = self.dim // self.num_heads
        self.num_key_value_groups = 1  # needed for eager attention
        self.qkv = nn.Linear(config.hidden_size, config.hidden_size * 3, bias=config.attention_bias)
        self.proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.scaling = self.head_dim**-0.5
        self.config = config
        self.attention_dropout = config.attention_dropout
        self.is_causal = False

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs,
    ) -> torch.Tensor:
        seq_length = hidden_states.shape[0]
        query_states, key_states, value_states = (
            self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)
        )
        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)

        query_states = query_states.transpose(0, 1).unsqueeze(0)
        key_states = key_states.transpose(0, 1).unsqueeze(0)
        value_states = value_states.transpose(0, 1).unsqueeze(0)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        if self.config._attn_implementation == "flash_attention_2":
            # Flash Attention 2: Use cu_seqlens for variable length attention
            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()
            attn_output, _ = attention_interface(
                self,
                query_states,
                key_states,
                value_states,
                attention_mask=None,
                scaling=self.scaling,
                dropout=0.0 if not self.training else self.attention_dropout,
                cu_seq_lens_q=cu_seqlens,
                cu_seq_lens_k=cu_seqlens,
                max_length_q=max_seqlen,
                max_length_k=max_seqlen,
                is_causal=False,
                **kwargs,
            )
        else:
            # Other implementations: Process each chunk separately
            lengths = cu_seqlens[1:] - cu_seqlens[:-1]

            tensor_dim = query_states.shape[2]  # query_states, key_states, value_states 维度 2 应该都一样

            # ✨ 重要：确保 lengths 是 Tensor 以便使用 .repeat()
            if not isinstance(lengths, torch.Tensor):
                lengths_tensor = torch.tensor(lengths, device=query_states.device) # 假设 lengths 是 list
            else:
                lengths_tensor = lengths

            lengths_sum = torch.sum(lengths).item() # 注意：如果 lengths 已经是 python list, 直接 sum(lengths)
            # print(f"[DEBUG Attention Split] Original tensor_dim: {tensor_dim}")
            # print(f"[DEBUG Attention Split] Original lengths_tensor shape: {lengths_tensor.shape}")
            # print(f"[DEBUG Attention Split] Original lengths_sum: {lengths_sum}")

            if tensor_dim != lengths_sum:
                num_repeats = tensor_dim // lengths_sum

                # 确保是整倍数
                if tensor_dim % lengths_sum != 0:
                    print(f"[ERROR Attention Split] Tensor dim {tensor_dim} is NOT a multiple of lengths sum {lengths_sum}!")
                    raise ValueError(
                        f"Tensor dim ({tensor_dim}) is not a multiple of "
                        f"lengths sum ({lengths_sum})"
                    )
                
                # print(f"[DEBUG Attention Split] Repeating lengths_tensor {num_repeats} times.")

                # 重复 'lengths' 张量以匹配合并后的输入
                # ✨ 使用 tensor 进行 repeat
                lengths_tensor = lengths_tensor.repeat(num_repeats)

                # print(f"[DEBUG Attention Split] Repeated lengths_tensor shape: {lengths_tensor.shape}")
                # print(f"[DEBUG Attention Split] Repeated lengths_sum: {torch.sum(lengths_tensor).item()}")

            final_split_list = lengths_tensor.tolist()
            # print(f"[DEBUG Attention Split] Final split list length: {len(final_split_list)}")
            # print(f"[DEBUG Attention Split] Final split list sum: {sum(final_split_list)}")
            splits = [
                torch.split(tensor, final_split_list, dim=2) for tensor in (query_states, key_states, value_states)
            ]

            attn_outputs = [
                attention_interface(
                    self,
                    q,
                    k,
                    v,
                    attention_mask=None,
                    scaling=self.scaling,
                    dropout=0.0 if not self.training else self.attention_dropout,
                    is_causal=False,
                    **kwargs,
                )[0]
                for q, k, v in zip(*splits)
            ]
            attn_output = torch.cat(attn_outputs, dim=1)

        attn_output = attn_output.reshape(seq_length, -1).contiguous()
        attn_output = self.proj(attn_output)
        return attn_output


class Glm4vVisionBlock(GradientCheckpointingLayer):
    def __init__(self, config) -> None:
        super().__init__()
        self.norm1 = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.norm2 = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.attn = Glm4vVisionAttention(config)
        self.mlp = Glm4VisionMlp(config, bias=False)

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs,
    ) -> torch.Tensor:
        hidden_states = hidden_states + self.attn(
            self.norm1(hidden_states),
            cu_seqlens=cu_seqlens,
            rotary_pos_emb=rotary_pos_emb,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))
        return hidden_states


class Glm4vTextRotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: Glm4vTextConfig, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        # In contrast to other models, Glm4vText has different position ids for the grids
        # So we expand the inv_freq to shape (3, ...)
        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)
        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


def rotate_half_llm(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., 0::2]
    x2 = x[..., 1::2]
    return torch.stack((-x2, x1), dim=-1).flatten(-2)


def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim=1):
    """Applies Rotary Position Embedding with Multimodal Sections to the query and key tensors (https://qwenlm.github.io/blog/qwen2-vl/).

    Explanation:
        Multimodal 3D rotary position embedding is an extension to 1D rotary position embedding. The input embedding
        sequence contains vision (images / videos) embedding and text embedding or just contains text embedding. For
        vision embedding part, we apply rotary position embedding on temporal, height and width dimension separately.
        Here we split the channel dimension to 3 chunks for the temporal, height and width rotary position embedding.
        For text embedding part, we just apply 1D rotary position embedding. The three rotary position index (temporal,
        height and width) of text embedding is always the same, so the text embedding rotary position embedding has no
        difference with modern LLMs.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        mrope_section(`List(int)`):
            Multimodal rope section is for channel dimension of temporal, height and width in rope calculation.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    mrope_section = mrope_section * 2
    cos = torch.cat([m[i % 3] for i, m in enumerate(cos.split(mrope_section, dim=-1))], dim=-1).unsqueeze(
        unsqueeze_dim
    )
    sin = torch.cat([m[i % 3] for i, m in enumerate(sin.split(mrope_section, dim=-1))], dim=-1).unsqueeze(
        unsqueeze_dim
    )

    # Interleave them instead of usual shape
    cos = cos[..., : cos.shape[-1] // 2].repeat_interleave(2, dim=-1)
    sin = sin[..., : sin.shape[-1] // 2].repeat_interleave(2, dim=-1)

    # Keep half or full tensor for later concatenation
    rotary_dim = cos.shape[-1]
    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]
    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]

    # Apply rotary embeddings on the first half or full tensor
    q_embed = (q_rot * cos) + (rotate_half_llm(q_rot) * sin)
    k_embed = (k_rot * cos) + (rotate_half_llm(k_rot) * sin)

    # Concatenate back to full shape
    q_embed = torch.cat([q_embed, q_pass], dim=-1)
    k_embed = torch.cat([k_embed, k_pass], dim=-1)

    return q_embed, k_embed


class Glm4vTextAttention(nn.Module):

    def __init__(self, config: Glm4vTextConfig, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.is_causal = True
        self.attention_dropout = config.attention_dropout
        self.rope_scaling = config.rope_scaling
        self.scaling = self.head_dim**-0.5

        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_multimodal_rotary_pos_emb(  # diff with Llama
            query_states, key_states, cos, sin, self.rope_scaling["mrope_section"]
        )

        if past_key_values is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            **kwargs,
        )

        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


class Glm4vTextMLP(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.config = config
        self.gate_up_proj = nn.Linear(config.hidden_size, 2 * config.intermediate_size, bias=False)
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)
        self.activation_fn = ACT2FN[config.hidden_act]

    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:
        up_states = self.gate_up_proj(hidden_states)

        gate, up_states = up_states.chunk(2, dim=-1)
        up_states = up_states * self.activation_fn(gate)

        return self.down_proj(up_states)


class Glm4vTextDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Glm4vTextConfig, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.self_attn = Glm4vTextAttention(config, layer_idx)
        self.mlp = Glm4vTextMLP(config)
        self.input_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_self_attn_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_mlp_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    @auto_docstring
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs,
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:
        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, _ = self.self_attn(
            hidden_states=hidden_states,
            position_embeddings=position_embeddings,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs,
        )

        hidden_states = self.post_self_attn_layernorm(hidden_states)
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = self.post_mlp_layernorm(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states


@dataclass
@auto_docstring(
    custom_intro="""
    Base class for Llava outputs, with hidden states and attentions.
    """
)
class Glm4vModelOutputWithPast(ModelOutput):
    r"""
    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).

        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
        `past_key_values` input) to speed up sequential decoding.
    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
        The rope index difference between sequence length and multimodal rope.
    """

    last_hidden_state: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Cache] = None
    hidden_states: Optional[tuple[torch.FloatTensor]] = None
    attentions: Optional[tuple[torch.FloatTensor]] = None
    rope_deltas: Optional[torch.LongTensor] = None


@auto_docstring
class Glm4vPreTrainedModel(PreTrainedModel):
    config: Glm4vConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Glm4vTextDecoderLayer", "Glm4vVisionBlock"]
    _skip_keys_device_placement = "past_key_values"
    _supports_flash_attn = True
    _supports_sdpa = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": Glm4vTextDecoderLayer,
        "attentions": Glm4vTextAttention,
    }

    def _init_weights(self, module):
        """
        Initialize the weights
        """
        std = self.config.initializer_range
        
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            if module.bias is not None:
                module.bias.data.zero_()
            if module.weight is not None:
                module.weight.data.fill_(1.0)


class Glm4vVisionModel(Glm4vPreTrainedModel):
    config: Glm4vVisionConfig
    _no_split_modules = ["Glm4vVisionBlock"]

    def __init__(self, config) -> None:
        super().__init__(config)
        self.spatial_merge_size = config.spatial_merge_size
        self.patch_size = config.patch_size

        self.embeddings = Glm4vVisionEmbeddings(config)
        self.patch_embed = Glm4vVisionPatchEmbed(config)

        head_dim = config.hidden_size // config.num_heads
        self.rotary_pos_emb = Glm4vVisionRotaryEmbedding(head_dim // 2)

        self.blocks = nn.ModuleList([Glm4vVisionBlock(config) for _ in range(config.depth)])
        self.merger = Glm4vVisionPatchMerger(
            dim=config.out_hidden_size, context_dim=config.intermediate_size, hidden_act=config.hidden_act
        )

        self.post_conv_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.downsample = nn.Conv2d(
            in_channels=config.hidden_size,
            out_channels=config.out_hidden_size,
            kernel_size=config.spatial_merge_size,
            stride=config.spatial_merge_size,
        )
        self.post_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        self.gradient_checkpointing = False
        self.post_init()

    def rot_pos_emb(self, grid_thw):
        pos_ids = []
        for t, h, w in grid_thw:
            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
            hpos_ids = hpos_ids.reshape(
                h // self.spatial_merge_size,
                self.spatial_merge_size,
                w // self.spatial_merge_size,
                self.spatial_merge_size,
            )
            hpos_ids = hpos_ids.permute(0, 2, 1, 3)
            hpos_ids = hpos_ids.flatten()

            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
            wpos_ids = wpos_ids.reshape(
                h // self.spatial_merge_size,
                self.spatial_merge_size,
                w // self.spatial_merge_size,
                self.spatial_merge_size,
            )
            wpos_ids = wpos_ids.permute(0, 2, 1, 3)
            wpos_ids = wpos_ids.flatten()
            pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
        pos_ids = torch.cat(pos_ids, dim=0)
        max_grid_size = grid_thw[:, 1:].max()
        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
        return rotary_pos_emb, pos_ids

    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, return_before_merge: bool = False) -> torch.Tensor:
        """
        Args:
            hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):
                The final hidden states of the model.
            grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):
                The temporal, height and width of feature shape of each image in LLM.

        Returns:
            `torch.Tensor`: hidden_states.
        """
        hidden_states = self.patch_embed(hidden_states)
        hidden_states = self.post_conv_layernorm(hidden_states)

        temporal_patch_size = self.config.temporal_patch_size
        if temporal_patch_size > 1:
        # 将 [[20, 24, 24]] 转换为 [[2, 24, 24]] × 10
            flattened_grid_thw = []
            for t, h, w in grid_thw:
                num_temporal_tokens = t.item() // temporal_patch_size
                repeated = torch.tensor(
                    [[1, h.item(), w.item()]] * num_temporal_tokens,
                    device=grid_thw.device,
                    dtype=grid_thw.dtype
                )
                flattened_grid_thw.append(repeated)
            grid_thw_for_pos = torch.cat(flattened_grid_thw, dim=0)
        else:
            grid_thw_for_pos = grid_thw

        rotary_pos_emb, image_type_ids = self.rot_pos_emb(grid_thw_for_pos)
        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)
        position_embeddings = (emb.cos(), emb.sin())

        cu_seqlens = torch.repeat_interleave(grid_thw_for_pos[:, 1] * grid_thw_for_pos[:, 2], grid_thw_for_pos[:, 0]).cumsum(
            dim=0,
            dtype=grid_thw_for_pos.dtype if torch.jit.is_tracing() else torch.int32,
        )
        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)
        seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()
        hidden_states = self.embeddings(hidden_states, seqlens, grid_thw_for_pos, image_type_ids[:, 0], image_type_ids[:, 1])

        for blk in self.blocks:
            hidden_states = blk(
                hidden_states,
                cu_seqlens=cu_seqlens,
                position_embeddings=position_embeddings,
            )

        hidden_states = self.post_layernorm(hidden_states)

        if return_before_merge:
            # Cubing Mode
            return hidden_states

        # Naive Mode
        hidden_states = hidden_states.view(
            -1, self.spatial_merge_size, self.spatial_merge_size, hidden_states.shape[-1]
        )
        hidden_states = hidden_states.permute(0, 3, 1, 2)
        hidden_states = self.downsample(hidden_states).view(-1, self.config.out_hidden_size)
        hidden_states = self.merger(hidden_states)
        return hidden_states


@auto_docstring
class Glm4vTextModel(Glm4vPreTrainedModel):
    config: Glm4vTextConfig

    def __init__(self, config: Glm4vTextConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Glm4vTextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Glm4vTextRotaryEmbedding(config=config)

        self.gradient_checkpointing = False
        self.post_init()

    @auto_docstring
    @check_model_inputs
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Union[tuple, BaseModelOutputWithPast]:
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if use_cache and past_key_values is None and not torch.jit.is_tracing():
            past_key_values = DynamicCache(config=self.config)

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)
        elif position_ids.ndim == 2:
            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)

        if position_ids.ndim == 3 and position_ids.shape[0] == 4:
            text_position_ids = position_ids[0]
            position_ids = position_ids[1:]
        else:
            text_position_ids = None

        mask_kwargs = {
            "config": self.config,
            "input_embeds": inputs_embeds,
            "attention_mask": attention_mask,
            "cache_position": cache_position,
            "past_key_values": past_key_values,
            "position_ids": text_position_ids,
        }
        causal_mask = create_causal_mask(**mask_kwargs)

        hidden_states = inputs_embeds

        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        for decoder_layer in self.layers:
            layer_outputs = decoder_layer(
                hidden_states,
                position_embeddings=position_embeddings,
                attention_mask=causal_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                cache_position=cache_position,
                **kwargs,
            )
            hidden_states = layer_outputs

        hidden_states = self.norm(hidden_states)

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values,
        )


@auto_docstring
class Glm4vModel(Glm4vPreTrainedModel):
    base_model_prefix = "model"
    _checkpoint_conversion_mapping = {}
    accepts_loss_kwargs = False
    config: Glm4vConfig
    _no_split_modules = ["Glm4vTextDecoderLayer", "Glm4vVisionBlock"]

    def __init__(self, config):
        super().__init__(config)
        self.visual = Glm4vVisionModel._from_config(config.vision_config)
        self.language_model = Glm4vTextModel._from_config(config.text_config)
        self.rope_deltas = None

        if config.use_cubing:
            self.cubing_module = Glm4vCubingModule(config)
            self.resampler = Glm4vResampler(config)
            self._current_lr_gumbel = config.cubing_lr_gumbel_start
        
        self.post_init()

        if hasattr(self, "_init_weights"): # Check if the method exists
            print("[INFO INIT WEIGHTS] Applying custom initialization to cubing_module...")
            if hasattr(self, 'cubing_module'):
                self.cubing_module.apply(self._init_weights)
            else:
                print("[WARNING INIT WEIGHTS] cubing_module not found for initialization.")

            print("[INFO INIT WEIGHTS] Applying custom initialization to resampler...")
            if hasattr(self, 'resampler'):
                self.resampler.apply(self._init_weights)
            else:
                print("[WARNING INIT WEIGHTS] resampler not found for initialization.")
        else:
            print("[WARNING INIT WEIGHTS] _init_weights method not found in Glm4vPreTrainedModel. Skipping explicit initialization.")

    
    def initialize_cubing_modules_if_needed(self):
        """智能初始化 Cubing 和 Resampler 模块"""
        if not hasattr(self, 'cubing_module'):
            return
        
        print("\n[CHECK] Verifying Cubing module initialization...")
        
        # ===== 检查所有关键参数 =====
        needs_init = self._check_resampler_params()
        
        if not needs_init:
            print("[INFO] All Cubing modules properly initialized\n")
            return
        
        print("[INFO] Initializing/Fixing Cubing modules...\n")
        
        # ===== 强制重新初始化所有参数 =====
        self._force_init_resampler()
        self._force_init_cubing()
        
        print("[INFO] Initialization complete!\n")

    def _check_resampler_params(self):
        """检查 Resampler 的所有参数是否正确初始化"""
        issues = []
        
        for name, param in self.resampler.named_parameters():
            device = param.device.type
            
            if device == 'meta':
                issues.append(f"{name}: on meta device")
                continue
            
            has_nan = torch.isnan(param).any()
            is_zero = (param.std() < 1e-6)
            
            if has_nan:
                issues.append(f"{name}: contains NaN")
            elif is_zero and 'bias' not in name and 'ln' not in name:
                # bias 和 LayerNorm 参数可以是 0
                issues.append(f"{name}: all zeros (std={param.std():.6f})")
        
        if issues:
            print("[ISSUES FOUND]")
            for issue in issues:
                print(f"  ❌ {issue}")
            return True
        
        return False

    def _force_init_resampler(self):
        """强制初始化 Resampler 的所有参数"""
        print("[INIT] Resampler modules:")
        
        # 1. 初始化所有 nn.Module
        for name, m in self.resampler.named_modules():
            if isinstance(m, nn.Linear):
                if m.weight.device.type != 'meta':
                    trunc_normal_(m.weight, std=0.02)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
                    print(f"  ✅ {name} (Linear)")
            
            elif isinstance(m, nn.LayerNorm):
                if m.weight.device.type != 'meta':
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0)
                    print(f"  ✅ {name} (LayerNorm)")
            
            elif isinstance(m, nn.MultiheadAttention):
                if hasattr(m, 'in_proj_weight') and m.in_proj_weight.device.type != 'meta':
                    nn.init.xavier_uniform_(m.in_proj_weight)
                    print(f"  ✅ {name}.in_proj_weight")
                
                if hasattr(m, 'in_proj_bias') and m.in_proj_bias.device.type != 'meta':
                    nn.init.constant_(m.in_proj_bias, 0)
                    print(f"  ✅ {name}.in_proj_bias")
        
        # 2. 初始化 nn.Parameter
        if self.resampler.query.device.type != 'meta':
            trunc_normal_(self.resampler.query, std=0.02)
            print(f"  ✅ query")
        
        if self.resampler.proj.device.type != 'meta':
            trunc_normal_(self.resampler.proj, std=0.02)
            print(f"  ✅ proj")

    def _force_init_cubing(self):
        """强制初始化 Cubing Module"""
        print("[INIT] Cubing module:")
        
        for name, m in self.cubing_module.named_modules():
            if isinstance(m, nn.Linear):
                if m.weight.device.type != 'meta':
                    trunc_normal_(m.weight, std=0.02)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
                    print(f"  ✅ {name}")


    def set_gumbel_noise(self, lr_gumbel: float):
        """Set current Gumbel noise level (called during training)"""
        if hasattr(self, 'cubing_module'):
            self._current_lr_gumbel = lr_gumbel

    def get_input_embeddings(self):
        return self.language_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.language_model.set_input_embeddings(value)


    def set_decoder(self, decoder):
        self.language_model = decoder

    def get_decoder(self):
        return self.language_model

    def get_rope_index(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        video_metadata: Optional[Glm4vCubingVideoMetadata] = None, 
        attention_mask: Optional[torch.Tensor] = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Calculate the 3D rope index
        """
        # import traceback
        # print(f"\n[DEBUG] get_rope_index called:")
        # print(f"  input_ids.shape: {input_ids.shape if input_ids is not None else None}")
        # print(f"  video_metadata: {video_metadata}")
        # print(f"\n[CALL STACK]:")
        # traceback.print_stack(limit=20)

        # ========== Cubing 模式：使用简化位置编码 ==========
        if video_metadata is not None and video_metadata.mode == "cubing":
            return self._get_rope_index_for_cubing(
                input_ids,
                video_metadata,
                attention_mask
            )
        
        # ========== Native 模式：原有逻辑 ==========
        spatial_merge_size = self.config.vision_config.spatial_merge_size
        image_token_id = self.config.image_token_id
        video_start_token_id = self.config.video_start_token_id
        video_end_token_id = self.config.video_end_token_id

        mrope_position_deltas = []
        if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):
            total_input_ids = input_ids
            if attention_mask is None:
                attention_mask = torch.ones_like(total_input_ids)
            position_ids = torch.ones(
                3,
                input_ids.shape[0],
                input_ids.shape[1],
                dtype=input_ids.dtype,
                device=input_ids.device,
            )
            image_index, video_index = 0, 0
            video_group_index = 0
            attention_mask = attention_mask.to(total_input_ids.device)
            for i, input_ids in enumerate(total_input_ids):
                input_ids = input_ids[attention_mask[i] == 1]
                input_tokens = input_ids.tolist()

                input_token_type = []
                video_check_flg = False
                for token in input_tokens:
                    if token == video_start_token_id:
                        video_check_flg = True
                    elif token == video_end_token_id:
                        video_check_flg = False

                    if token == image_token_id and not video_check_flg:
                        input_token_type.append("image")
                    elif token == image_token_id and video_check_flg:
                        input_token_type.append("video")
                    else:
                        input_token_type.append("text")

                input_type_group = []
                for key, group in itertools.groupby(enumerate(input_token_type), lambda x: x[1]):
                    group = list(group)
                    start_index = group[0][0]
                    end_index = group[-1][0] + 1
                    input_type_group.append((key, start_index, end_index))

                llm_pos_ids_list = []
                video_frame_num = 1
                for modality_type, start_idx, end_idx in input_type_group:
                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0

                    if modality_type == "image":
                        t, h, w = (
                            image_grid_thw[image_index][0],
                            image_grid_thw[image_index][1],
                            image_grid_thw[image_index][2],
                        )
                        llm_grid_t, llm_grid_h, llm_grid_w = (
                            t.item(),
                            h.item() // spatial_merge_size,
                            w.item() // spatial_merge_size,
                        )

                        t_index = torch.arange(llm_grid_t).view(-1, 1).expand(-1, llm_grid_h * llm_grid_w).flatten()
                        h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(llm_grid_t, -1, llm_grid_w).flatten()
                        w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()
                        llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + st_idx)

                        image_index += 1
                        video_frame_num = 1

                    elif modality_type == "video":
                        t, h, w = (
                            video_frame_num,
                            video_grid_thw[video_index][1],
                            video_grid_thw[video_index][2],
                        )

                        llm_grid_t, llm_grid_h, llm_grid_w = (
                            t,
                            h.item() // spatial_merge_size,
                            w.item() // spatial_merge_size,
                        )

                        for t_idx in range(llm_grid_t):
                            t_index = torch.tensor(t_idx).view(-1, 1).expand(-1, llm_grid_h * llm_grid_w).flatten()

                            h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(1, -1, llm_grid_w).flatten()
                            w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(1, llm_grid_h, -1).flatten()
                            llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + st_idx)

                        video_group_index += 1

                        if video_group_index >= video_grid_thw[video_index][0]:
                            video_index += 1
                            video_group_index = 0

                        video_frame_num += 1

                    else:
                        text_len = end_idx - start_idx
                        llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)

                        video_frame_num = 1

                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)
                mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))
            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)
            return position_ids, mrope_position_deltas
        else:
            if attention_mask is not None:
                position_ids = attention_mask.long().cumsum(-1) - 1
                position_ids.masked_fill_(attention_mask == 0, 1)
                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(attention_mask.device)
                max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]
                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
            else:
                position_ids = (
                    torch.arange(input_ids.shape[1], device=input_ids.device)
                    .view(1, 1, -1)
                    .expand(3, input_ids.shape[0], -1)
                )
                mrope_position_deltas = torch.zeros(
                    [input_ids.shape[0], 1],
                    device=input_ids.device,
                    dtype=input_ids.dtype,
                )

            return position_ids, mrope_position_deltas

    def _get_rope_index_for_cubing(
        self,
        input_ids: torch.LongTensor,
        video_metadata: Glm4vCubingVideoMetadata,
        attention_mask: Optional[torch.Tensor] = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Cubing 模式的 RoPE 位置编码
        """
        batch_size = input_ids.shape[0]
        seq_len = input_ids.shape[1]
        device = input_ids.device
        # print(f"[DEBUG ROPE] input_ids: {input_ids}")
        # print(f"\n[DEBUG Padding Check] ===== Start =====")
        # print(f"  input_ids shape: {input_ids.shape}")
        # print(f"  attention_mask shape: {attention_mask.shape if attention_mask is not None else None}")
        
        # if attention_mask is not None:
        #     # 找到第一个 0 的位置
        #     for i in range(batch_size):
        #         mask_values = attention_mask[i]
        #         first_zero_idx = (mask_values == 0).nonzero(as_tuple=True)
        #         if len(first_zero_idx[0]) > 0:
        #             first_zero_pos = first_zero_idx[0][0].item()
        #             print(f"  Batch {i}: first padding at position {first_zero_pos}")
        #             print(f"    Mask values around that position: {mask_values[first_zero_pos-3:first_zero_pos+3]}")
        #             print(f"    input_ids at padding positions: {input_ids[i, first_zero_pos:first_zero_pos+5]}")
        #         else:
        #             print(f"  Batch {i}: No padding (all 1s)")
        #             print(f"    Sequence length: {(mask_values == 1).sum().item()}")
        
        # print(f"  pad_token_id: {self.config.pad_token_id}")
        # print(f"[DEBUG Padding Check] ===== End =====\n")
        
        video_token_id = self.config.video_token_id
        video_start_token_id = self.config.video_start_token_id
        video_end_token_id = self.config.video_end_token_id
        
        # 初始化 position_ids（默认全1，padding 位置会保持1）
        position_ids = torch.ones(
            3, batch_size, seq_len,
            dtype=torch.long,
            device=device
        )
        
        mrope_position_deltas = []
        
        # 预先计算 cube 位置
        cube_positions = None
        if video_metadata is not None and video_metadata.cube_bounds:
            cube_positions = self._allocate_cube_positions(video_metadata, device)

        # print(f"[DEBUG ROPE] cube_positions: {cube_positions}")
        
        for i in range(batch_size):
            # ========== 关键修复：直接遍历整个序列 ==========
            st_idx = 0
            video_token_idx = 0
            in_video_section = False
            video_start_pos = None
            
            # 遍历完整序列（而不是 valid_tokens）
            for j in range(seq_len):
                # 检查是否是有效 token
                if attention_mask is not None and attention_mask[i, j] == 0:
                    # Padding 位置，保持默认值 1
                    position_ids[:, i, j] = 1
                    continue
                
                token = input_ids[i, j]
                
                # 处理 <|video_start|>
                if token == video_start_token_id:
                    in_video_section = True
                    video_start_pos = st_idx
                    position_ids[:, i, j] = st_idx
                    st_idx += 1
                    continue
                
                # 处理 <|video_end|>
                elif token == video_end_token_id:
                    in_video_section = False
                    position_ids[:, i, j] = st_idx
                    st_idx += 1
                    continue
                
                # 处理 <|video|> tokens
                if token == video_token_id and in_video_section:
                    if cube_positions is not None and video_token_idx < len(cube_positions):
                        # 使用 cube 分配的相对位置
                        relative_pos = cube_positions[video_token_idx]
                        actual_pos = video_start_pos + 1 + relative_pos
                        
                        position_ids[0, i, j] = actual_pos
                        position_ids[1, i, j] = actual_pos
                        position_ids[2, i, j] = actual_pos
                        
                        video_token_idx += 1
                        st_idx = max(st_idx, actual_pos + 1)
                    else:
                        # 降级：递增
                        position_ids[:, i, j] = st_idx
                        st_idx += 1
                
                # 处理其他 tokens（文本、图像等）
                else:
                    position_ids[:, i, j] = st_idx
                    st_idx += 1
            
            # 计算 delta
            max_pos = position_ids[:, i, :].max()
            delta = max_pos + 1 - seq_len
            mrope_position_deltas.append(delta)
        
        mrope_position_deltas = torch.tensor(
            mrope_position_deltas,
            device=device,
            dtype=torch.long
        ).unsqueeze(1)

        # print(f"\n[DEBUG CubingRoPE Final] ----- Start -----")
        # 找到第一个视频 token 的索引
        # first_video_token_idx = -1
        # last_video_token_idx = -1
        # if video_token_id in input_ids[0]: # Assuming batch size 1 for simplicity
        #      video_indices = (input_ids[0] == video_token_id).nonzero(as_tuple=True)[0]
        #      if len(video_indices) > 0:
        #           first_video_token_idx = video_indices[0].item()
        #           last_video_token_idx = video_indices[-1].item()

        # print(f"  Final position_ids shape: {position_ids.shape}")
        # print(f"  Final rope_deltas: {mrope_position_deltas}")
        # if first_video_token_idx != -1:
        #      print(f"  Position IDs for first 5 video tokens (at index {first_video_token_idx}): {position_ids[:, 0, first_video_token_idx : first_video_token_idx+5]}")
        #      print(f"  Position IDs for last 5 video tokens (at index {last_video_token_idx-4}): {position_ids[:, 0, last_video_token_idx-4 : last_video_token_idx+1]}")
        # print(f"  Position IDs for tokens overall: {position_ids[:, 0, :]}")
        # # print(f"  Position IDs for last 10 tokens overall (seq_len={seq_len}): {position_ids[:, 0, -10:]}")
        # print(f"[DEBUG CubingRoPE Final] ----- End -----")
        
        return position_ids, mrope_position_deltas

    def _allocate_cube_positions(
        self,
        video_metadata: Glm4vCubingVideoMetadata,
        device: torch.device
    ) -> torch.Tensor:
        """
        为每个 video token 分配位置（相对于 video_start 的偏移）
        
        策略：按 cube 的帧数比例分配位置范围
        """
        cube_bounds = video_metadata.cube_bounds[0]
        total_tokens = video_metadata.actual_num_tokens
        
        # 计算每个 cube 的帧数
        cube_frames = []
        for start_frame, end_frame in cube_bounds:
            num_frames = end_frame - start_frame
            cube_frames.append(num_frames)
        
        total_frames = sum(cube_frames)
        num_cubes = len(cube_frames)
        
        # 按比例分配位置数
        position_allocations = []
        for num_frames in cube_frames:
            allocated = int(round(total_tokens * num_frames / total_frames))
            position_allocations.append(allocated)
        
        # 调整确保总和 = total_tokens
        diff = total_tokens - sum(position_allocations)
        if diff != 0:
            position_allocations[-1] += diff
        
        # 计算边界
        boundaries = [0]
        for alloc in position_allocations:
            boundaries.append(boundaries[-1] + alloc)
        
        # 为每个 cube 分配 tokens
        tokens_per_cube = total_tokens // num_cubes
        remaining = total_tokens % num_cubes
        
        # 生成位置（从 0 开始的相对位置）
        all_positions = []
        
        for cube_idx in range(num_cubes):
            # 当前 cube 的 token 数
            if cube_idx < remaining:
                current_tokens = tokens_per_cube + 1
            else:
                current_tokens = tokens_per_cube
            
            # 位置范围
            start_pos = boundaries[cube_idx]
            end_pos = boundaries[cube_idx + 1] - 1
            
            # 生成位置
            if current_tokens > 0:
                cube_pos = torch.linspace(
                    start_pos, 
                    end_pos, 
                    current_tokens,
                    device=device
                ).long()
                all_positions.append(cube_pos)
        
        if len(all_positions) > 0:
            positions = torch.cat(all_positions, dim=0)
        else:
            positions = torch.tensor([], dtype=torch.long, device=device)
        

        # print(f"\n[DEBUG AllocatePos] ----- Start -----")
        # print(f"  Input total_tokens: {total_tokens}")
        # print(f"  Input cube_bounds: {cube_bounds}")
        # print(f"  Calculated cube_frames: {cube_frames}")
        # print(f"  Calculated total_frames: {total_frames}")
        # print(f"  Position allocations per cube: {position_allocations}")
        # print(f"  Calculated boundaries: {boundaries}")
        # print(f"  Final generated relative positions (first 64): {positions[:64]}")
        # print(f"  Final generated relative positions shape: {positions.shape}")
        # print(f"[DEBUG AllocatePos] ----- End -----")
        return positions

    def get_video_features(
        self,
        pixel_values_videos: torch.FloatTensor,
        video_grid_thw: torch.LongTensor,
        videos_bound: Optional[list] = None,
    ) -> Tuple[tuple, Glm4vCubingVideoMetadata]:
        """
        Encodes videos into continuous embeddings
        
        Returns:
            video_embeds: tuple of tensors
            video_metadata: Glm4vCubingVideoMetadata object
        """
        # print(f"[DEBUG NAN features] get_video_features called")
        # print(f"[DEBUG NAN features] pixel_values_videos: {pixel_values_videos}, shape: {pixel_values_videos.shape}")
        # print(f"[DEBUG NAN features] video_grid_thw: {video_grid_thw}, shape: {video_grid_thw.shape}")
        # print(f"[DEBUG NAN features] videos_bound: {videos_bound}")
        pixel_values_videos = pixel_values_videos.type(self.visual.dtype)
        
        video_metadata = Glm4vCubingVideoMetadata(
            original_grid_thw=video_grid_thw,
            mode="cubing" if self.config.use_cubing else "native",
            temporal_patch_size=self.config.vision_config.temporal_patch_size
        )
        # print(f"[DEBUG NAN features] video_matadata: {video_metadata}")

        temporal_patch_size = self.config.vision_config.temporal_patch_size
        # if temporal_patch_size > 1 and videos_bound is not None:
        #     videos_bound = [(start // temporal_patch_size, end // temporal_patch_size) 
        #                     for start, end in videos_bound]
        # print(f"[DEBUG NAN features] temporal_patch_size: {temporal_patch_size}")
        
        if self.config.use_cubing:
            video_embeds, video_metadata = self._get_video_features_with_cubing(
                pixel_values_videos,
                video_metadata,
                videos_bound
            )
            # print(f"  video_embeds after _get_video_features_with_cubing Shape: {video_embeds[0].shape}, dtype: {video_embeds[0].dtype}")
            # print(f"  Has NaN: {torch.isnan(video_embeds[0]).any()}") #true 
            # print(f"  Has Inf: {torch.isinf(video_embeds[0]).any()}")

        else:
            video_embeds = self._get_video_features_native(
                pixel_values_videos,
                video_metadata
            )
        
        
        return video_embeds, video_metadata

    def _get_video_features_native(
        self,
        pixel_values_videos: torch.FloatTensor,
        video_metadata: Glm4vCubingVideoMetadata,
    ) -> tuple:
        """Native GLM4V video processing"""
        temporal_patch_size = self.config.vision_config.temporal_patch_size
        flattened_grid_thw = video_metadata.to_flattened()
        
        video_embeds = self.visual(
            pixel_values_videos,
            grid_thw=flattened_grid_thw,
            return_before_merge=False
        )
        
        split_sizes = (
            video_metadata.original_grid_thw.prod(-1) 
            // self.visual.spatial_merge_size ** 2 * temporal_patch_size
        ).tolist()
        video_embeds = torch.split(video_embeds, split_sizes)
        
        video_metadata.actual_num_tokens = sum(split_sizes)
        
        return video_embeds

    def _get_video_features_with_cubing(
        self,
        pixel_values_videos: torch.FloatTensor,
        video_metadata: Glm4vCubingVideoMetadata,
        videos_bound: Optional[list] = None, # 这里应该是原始的videos_bound
    ) -> Tuple[tuple, Glm4vCubingVideoMetadata]:
        """Video processing with Cubing technique"""
        # temporal_patch_size = self.config.vision_config.temporal_patch_size
        
        if videos_bound is None:
            videos_bound = []
            current_frame_idx = 0
            for t, h, w in video_metadata.original_grid_thw:
                num_frames = t.item()
                videos_bound.append((current_frame_idx, current_frame_idx + num_frames))
                current_frame_idx += num_frames

        # print(f"[DEBUG NAN features] videos_bound: {videos_bound}")
        
        # # 转换为 temporal token 级别
        # if temporal_patch_size > 1:
        #     videos_bound_tokens = [(start // temporal_patch_size, end // temporal_patch_size) 
        #                             for start, end in videos_bound]
        # else:
        #     videos_bound_tokens = videos_bound

        # print(f"[DEBUG NAN features] videos_bound_tokens: {videos_bound_tokens}")
        
        
        # ========== 原有逻辑不变 ========== 
        flattened_grid_thw = video_metadata.to_flattened()

        vision_features = self.visual(
            pixel_values_videos,
            grid_thw=flattened_grid_thw,
            return_before_merge=True
        )
        
        frame_features = self._reconstruct_frames(
            vision_features,
            video_metadata.original_grid_thw
        )
        
        video_embeds_list = []
        all_cube_bounds = []
        all_gate_logits = []
        

        # print(f"[DEBUG NAN features] videos_bound_tokens: {videos_bound}")
        # print(f"[DEBUG NAN features] frame_features shape: {len(frame_features)} ,{frame_features[0].shape}")
        for video_idx, (vs, ve) in enumerate(videos_bound):
            video_frames = torch.stack([
                frame_features[i] for i in range(vs, ve)
            ])
            # print(f"[DEBUG NAN features] video_frames_for cubing: {video_frames}")
            # print(f"  LLM output has NaN: {torch.isnan(video_frames).any()}")
            # print(f"  LLM output has Inf: {torch.isinf(video_frames).any()}")
            
            cubing_result = self.cubing_module(
                video_frames,
                fpq=self.config.cubing_fpq,
                temperature=self.config.cubing_temperature,
                lr_gumbel=self._current_lr_gumbel,
            )
            # print(f"[DEBUG NAN features] cubing_result: {cubing_result}")
            # print(f"  LLM output has NaN: {torch.isnan(cubing_result).any()}")
            # print(f"  LLM output has Inf: {torch.isinf(cubing_result).any()}")
            
            all_gate_logits.append(cubing_result['gate_logits'])
            all_cube_bounds.append(cubing_result['cube_bounds'])
            
            cube_tokens = []
            h_patches = video_metadata.original_grid_thw[video_idx][1].item()
            w_patches = video_metadata.original_grid_thw[video_idx][2].item()
            
            for start, end in cubing_result['cube_bounds']:
                cube = video_frames[start:end]
                cube_flat = cube.reshape(-1, 1536)
                # print(f"  Before Resampler has NaN: {torch.isnan(cube_flat).any()}")
                # print(f"  Before Resampler has Inf: {torch.isinf(cube_flat).any()}")
    
                tokens = self.resampler(
                    cube_flat,
                    tgt_size_range=[
                        [start, end],
                        [0, h_patches],
                        [0, w_patches]
                    ]
                )
                # print(f"  After Resampler has NaN: {torch.isnan(tokens).any()}")
                # print(f"  After Resampler has Inf: {torch.isinf(tokens).any()}")
                cube_tokens.append(tokens)
            
            if cubing_result['thumbnail'] is not None:
                cube_tokens.insert(0, cubing_result['thumbnail'].unsqueeze(0))
            
            video_tokens = torch.cat(cube_tokens, dim=0)
            video_embeds_list.append(video_tokens)
        
        total_tokens = sum(v.shape[0] for v in video_embeds_list)
        video_metadata.actual_num_tokens = total_tokens
        video_metadata.cube_bounds = all_cube_bounds
        
        if self.training and all_gate_logits:
            self._last_gate_logits = torch.cat(all_gate_logits, dim=0)

        # print(f"[DEBUG features cubing] video_embeds_list: {len(video_embeds_list)}, {video_embeds_list[0].shape}")
        # print(f"[DEBUG features cubing] video_metadata: {video_metadata}")
        # print(f"  LLM output has NaN: {torch.isnan(video_embeds_list[0]).any()}")
        # print(f"  LLM output has Inf: {torch.isinf(video_embeds_list[0]).any()}")
        
        return tuple(video_embeds_list), video_metadata

    def _reconstruct_frames(
        self,
        flat_features: torch.Tensor,
        grid_thw: torch.Tensor 
    ) -> list:
        """
        Reconstruct flattened features into frame format
        """
        frames = []
        start_idx = 0
        
        for t, h, w in grid_thw:
            num_temporal_tokens = t 
            seq_len = num_temporal_tokens * h * w

            feat = flat_features[start_idx:start_idx + seq_len]
            feat = feat.reshape(num_temporal_tokens, h * w, -1)
            
            for frame_idx in range(num_temporal_tokens):
                frames.append(feat[frame_idx])
            
            start_idx += seq_len
        
        return frames

    def get_cubing_aux_loss(self, alpha=0.001):
        """Calculate auxiliary loss for Cubing gate network"""
        if hasattr(self, '_last_gate_logits'):
            gate_logits = self._last_gate_logits
            aux_loss = alpha * torch.norm(gate_logits, p=2)
            delattr(self, '_last_gate_logits')
            return aux_loss
        return torch.tensor(0.0, device=self.visual.patch_embed.proj.weight.device)

    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):
        """Encodes images into continuous embeddings"""
        pixel_values = pixel_values.type(self.visual.dtype)
        image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)
        split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
        image_embeds = torch.split(image_embeds, split_sizes)
        return image_embeds

    def get_placeholder_mask(
        self,
        input_ids: torch.LongTensor,
        inputs_embeds: torch.FloatTensor,
        image_features: Optional[torch.FloatTensor] = None,
        video_features: Optional[torch.FloatTensor] = None,
    ):
        """Obtains multimodal placeholder mask"""
        if input_ids is None:
            special_image_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)
            )
            special_image_mask = special_image_mask.all(-1)
            special_video_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)
            )
            special_video_mask = special_video_mask.all(-1)
        else:
            special_image_mask = input_ids == self.config.image_token_id
            special_video_mask = input_ids == self.config.video_token_id

        n_image_tokens = special_image_mask.sum()
        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():
            raise ValueError(
                f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}"
            )

        n_video_tokens = special_video_mask.sum()
        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():
            raise ValueError(
                f"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}"
            )

        return special_image_mask, special_video_mask
    
    @auto_docstring
    @can_return_tuple
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,          # Shape [B, L_orig]
        attention_mask: Optional[torch.Tensor] = None,         # Shape [B, L_orig] or dict
        position_ids: Optional[torch.LongTensor] = None,       # Shape [3, B, L_orig] - Calculated by Collator, WE WILL IGNORE THIS
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,     # Shape [B, L_orig, D]
        pixel_values: Optional[torch.Tensor] = None,
        pixel_values_videos: Optional[torch.FloatTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        videos_bound: Optional[list] = None,
        rope_deltas: Optional[torch.LongTensor] = None,       # Calculated by Collator, WE WILL RECALCULATE
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, Glm4vModelOutputWithPast]:

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        # ========== Step 1: 获取初始文本 Embeddings ==========
        # print(f"===STEP 1 Getting initial inputs and embeddings===")
        if inputs_embeds is None:
            # print(f"\n[DEBUG NAN SRC] Checking expanded input_ids before embedding:")
            # print(f"  Shape: {input_ids.shape}, dtype: {input_ids.dtype}")
            # print(f"  Min ID: {input_ids.min()}, Max ID: {input_ids.max()}") # Check for invalid IDs
            initial_inputs_embeds = self.get_input_embeddings()(input_ids) # Shape [B, L_orig, D]
        else:
            initial_inputs_embeds = inputs_embeds # Shape [B, L_orig, D]
        # print(f"initial video_grid_thw: {video_grid_thw}")

        # print(f"\n[DEBUG NAN SRC] Initial Text Embeddings:")
        # print(f"  Shape: {initial_inputs_embeds.shape}, dtype: {initial_inputs_embeds.dtype}")
        # print(f"  Has NaN: {torch.isnan(initial_inputs_embeds).any()}")
        # print(f"  Has Inf: {torch.isinf(initial_inputs_embeds).any()}")

        # ========== Step 2: 获取并插入图像特征 (如果存在) ==========
        # print(f"===STEP 2 Getting image features===")
        # (假设图像特征已由 Plugin 正确插入或由 get_placeholder_mask 处理)
        # 注意：如果图像也需要动态插入，此逻辑需要合并/调整
        current_inputs_embeds = initial_inputs_embeds
        if pixel_values is not None:
            # 假设 image 特征已通过 masked_scatter 插入 initial_inputs_embeds
            # (省略 image 插入代码，假定它在前面已完成或在此处完成)
            image_embeds = self.get_image_features(pixel_values, image_grid_thw)
            image_embeds = torch.cat(image_embeds, dim=0).to(current_inputs_embeds.device, current_inputs_embeds.dtype)
            # print(f"\n[DEBUG NAN SRC] Image Embeddings (before scatter):")
            # print(f"  Shape: {image_embeds.shape}, dtype: {image_embeds.dtype}")
            # print(f"  Has NaN: {torch.isnan(image_embeds).any()}")
            # print(f"  Has Inf: {torch.isinf(image_embeds).any()}")

            image_mask, _ = self.get_placeholder_mask(input_ids, current_inputs_embeds, image_features=image_embeds) # Assuming get_placeholder_mask works for images
            current_inputs_embeds = current_inputs_embeds.masked_scatter(image_mask, image_embeds)
            # print(f"[DEBUG FORWARD V2] Applied image features scatter.")
            # print(f"\n[DEBUG NAN SRC] Embeddings After Image Scatter:")
            # print(f"  Shape: {current_inputs_embeds.shape}, dtype: {current_inputs_embeds.dtype}")
            # print(f"  Has NaN: {torch.isnan(current_inputs_embeds).any()}")
            # print(f"  Has Inf: {torch.isinf(current_inputs_embeds).any()}")


        # ========== Step 3: 获取视频特征 ==========
        # print(f"===STEP 3 Getting video features===")
        video_embeds_list = []      # List of [num_tokens_video_i, D]
        video_metadata = None       # Metadata for position calculation (assume single video per batch for simplicity in example)
        actual_num_video_tokens = 0 # Total video tokens generated

        if pixel_values_videos is not None:
            # --- 获取视频特征 ---
            video_embeds_tuple, video_metadata = self.get_video_features(
                pixel_values_videos,
                video_grid_thw,
                videos_bound=videos_bound
            )
            # 假设 video_embeds_tuple 只包含一个视频的特征张量 (简化处理)

            video_embeds_list = [v.to(current_inputs_embeds.device, current_inputs_embeds.dtype) for v in video_embeds_tuple]
            actual_num_video_tokens = video_embeds_list[0].shape[0] # Assuming single video for now
            # print(f"[DEBUG FORWARD V2] Got video features. Actual video tokens: {actual_num_video_tokens}")
            # print(f"  Video metadata: {video_metadata}")

            # print(f"video_embeds_tuple: {video_embeds_tuple}, video_metadata: {video_metadata}")
            # print(f"\n[DEBUG NAN SRC] Embeddings After Image Scatter:")
            # print(f"  Shape: {video_embeds_tuple[0].shape}, dtype: {video_embeds_tuple[0].dtype}")
            # print(f"  Has NaN: {torch.isnan(video_embeds_tuple[0]).any()}")
            # print(f"  Has Inf: {torch.isinf(video_embeds_tuple[0]).any()}")


        # ========== Step 4: 动态构建最终 Embeddings 和 Mask ==========
        # print(f"===STEP 4 Building final embeddings and masks===")

        if video_embeds_list:
            # 获取 start/end token 的 embeddings
            video_start_embed = self.get_input_embeddings()(
                torch.tensor(self.config.video_start_token_id, device=current_inputs_embeds.device)
            ).unsqueeze(0)  # [1, D]
            
            video_end_embed = self.get_input_embeddings()(
                torch.tensor(self.config.video_end_token_id, device=current_inputs_embeds.device)
            ).unsqueeze(0)  # [1, D]
            
            final_inputs_embeds_list = []
            final_attention_mask_list = []
            batch_size, original_seq_len = input_ids.shape
            video_placeholder_id = self.config.video_token_id
            
            # print(f"[DEBUG STEP4] Original input_ids shape: {input_ids.shape}")
            # print(f"[DEBUG STEP4] video_start_token_id: {self.config.video_start_token_id}")
            # print(f"[DEBUG STEP4] video_end_token_id: {self.config.video_end_token_id}")

            for i in range(batch_size):
                video_positions = (input_ids[i] == video_placeholder_id).nonzero(as_tuple=True)[0]
                # print(f"[DEBUG STEP4] Batch {i} video_positions: {video_positions}")

                if len(video_positions) == 0:
                    # 没有视频，直接使用原始 embeds 和 mask
                    final_inputs_embeds_list.append(current_inputs_embeds[i])
                    final_attention_mask_list.append(
                        attention_mask[i] if attention_mask is not None 
                        else torch.ones(original_seq_len, device=input_ids.device)
                    )
                    continue
                
                if len(video_positions) > 1:
                    print(f"[WARNING STEP4] Multiple video placeholders in batch {i}, using first one")

                video_pos = video_positions[0].item()
                current_video_embeds = video_embeds_list[i]  # [num_video_tokens, D]
                num_video_tokens = current_video_embeds.shape[0]
                
                # print(f"[DEBUG STEP4] Batch {i}: video_pos={video_pos}, num_video_tokens={num_video_tokens}")
                
                # ✨ 关键：构建包含 start/end 的序列
                new_embeds = torch.cat([
                    current_inputs_embeds[i, :video_pos],      # 前面部分
                    video_start_embed,                         # <|video_start|>
                    current_video_embeds,                      # N个视频tokens
                    video_end_embed,                           # <|video_end|>
                    current_inputs_embeds[i, video_pos + 1:]   # 后面部分
                ], dim=0)
                final_inputs_embeds_list.append(new_embeds)
                
                # 构建新的 mask（长度 = 原始长度 - 1 + 1(start) + N(video) + 1(end)）
                num_tokens_to_insert = num_video_tokens + 2  # +2 for start/end
                
                if attention_mask is not None:
                    current_mask = attention_mask[i]
                    new_mask = torch.cat([
                        current_mask[:video_pos],
                        torch.ones(num_tokens_to_insert, dtype=current_mask.dtype, device=current_mask.device),
                        current_mask[video_pos + 1:]
                    ], dim=0)
                    final_attention_mask_list.append(new_mask)
                else:
                    new_len = original_seq_len - 1 + num_tokens_to_insert
                    final_attention_mask_list.append(
                        torch.ones(new_len, dtype=torch.long, device=input_ids.device)
                    )
                
                # print(f"[DEBUG STEP4] Batch {i} new sequence length: {new_embeds.shape[0]}")

            # --- 填充 Batch ---
            max_len_new = max(embed.shape[0] for embed in final_inputs_embeds_list)
            final_inputs_embeds = torch.zeros(
                batch_size, max_len_new, current_inputs_embeds.shape[2], 
                dtype=current_inputs_embeds.dtype, 
                device=current_inputs_embeds.device
            )
            final_attention_mask = torch.zeros(
                batch_size, max_len_new, 
                dtype=torch.long, 
                device=input_ids.device
            )

            for i in range(batch_size):
                seq_len_i = final_inputs_embeds_list[i].shape[0]
                final_inputs_embeds[i, :seq_len_i] = final_inputs_embeds_list[i]
                final_attention_mask[i, :seq_len_i] = final_attention_mask_list[i]

            # print(f"[DEBUG STEP4] After dynamic insertion:")
            # print(f"  final_inputs_embeds shape: {final_inputs_embeds.shape}")
            # print(f"  final_attention_mask shape: {final_attention_mask.shape}")
            # print(f"  Expected length: original({original_seq_len}) - 1 + 2 + video_tokens = {original_seq_len + 1 + num_video_tokens}")

        else:
            # 没有视频，直接使用原始/图像插入后的 embeds 和 mask
            final_inputs_embeds = current_inputs_embeds
            final_attention_mask = attention_mask if attention_mask is not None else torch.ones_like(input_ids)
            max_len_new = final_inputs_embeds.shape[1]
            # print(f"[DEBUG STEP4] No video insertion needed.")


        # ========== Step 5: ✨ 重新计算 Position IDs ==========
        # print(f"===STEP 5 Recalculating Position IDs===")

        final_position_ids = None
        final_rope_deltas = None

        if video_embeds_list:
            actual_num_video_tokens_per_video_recalc = [v.shape[0] for v in video_embeds_list]
            # print(f"[DEBUG STEP5] actual_num_video_tokens_per_video: {actual_num_video_tokens_per_video_recalc}")
            # print(f"[DEBUG STEP5] Recalculating position IDs for new length {max_len_new}")

            # 构建临时 input_ids（包含 start/end tokens）
            temp_input_ids = torch.full(
                (batch_size, max_len_new), 
                self.config.pad_token_id or 0, 
                dtype=torch.long, 
                device=input_ids.device
            )

            video_placeholder_id = self.config.video_token_id
            video_start_token_id = self.config.video_start_token_id
            video_end_token_id = self.config.video_end_token_id

            for i in range(batch_size):
                original_len_i = input_ids[i].shape[0]
                video_positions = (input_ids[i] == video_placeholder_id).nonzero(as_tuple=True)[0]
                
                if len(video_positions) > 0:
                    video_pos_orig = video_positions[0].item()
                    
                    if i >= len(actual_num_video_tokens_per_video_recalc):
                        raise IndexError(
                            f"Batch index {i} out of range for video tokens list "
                            f"(len={len(actual_num_video_tokens_per_video_recalc)})"
                        )
                    
                    num_video_tokens_inserted = actual_num_video_tokens_per_video_recalc[i]
                    
                    # ✨ 关键：构建包含 start/video/end 的序列
                    # 前面部分（不变）
                    temp_input_ids[i, :video_pos_orig] = input_ids[i, :video_pos_orig]
                    
                    # 插入 <|video_start|>
                    temp_input_ids[i, video_pos_orig] = video_start_token_id
                    
                    # 插入 N 个 <|video|> tokens
                    video_token_start = video_pos_orig + 1
                    video_token_end = video_token_start + num_video_tokens_inserted
                    temp_input_ids[i, video_token_start:video_token_end] = video_placeholder_id
                    
                    # 插入 <|video_end|>
                    temp_input_ids[i, video_token_end] = video_end_token_id
                    
                    # 复制后续部分
                    orig_after_start = video_pos_orig + 1  # 原始序列中 placeholder 后的位置
                    new_after_start = video_token_end + 1  # 新序列中 end token 后的位置
                    len_after = original_len_i - orig_after_start
                    
                    if len_after > 0:
                        end_idx_temp = min(new_after_start + len_after, max_len_new)
                        end_idx_orig = min(orig_after_start + len_after, original_len_i)
                        temp_input_ids[i, new_after_start:end_idx_temp] = input_ids[i, orig_after_start:end_idx_orig]
                    
                    # 验证输出
                    # print(f"[DEBUG STEP5] Batch {i} temp_input_ids structure:")
                    # print(f"  Position {video_pos_orig}: {temp_input_ids[i, video_pos_orig]} (expect start={video_start_token_id})")
                    # print(f"  Positions {video_token_start}-{video_token_end-1}: video tokens")
                    # print(f"  Position {video_token_end}: {temp_input_ids[i, video_token_end]} (expect end={video_end_token_id})")
                    # print(f"  Sample: {temp_input_ids[i, max(0, video_pos_orig-2):min(video_token_end+3, max_len_new)]}")
                
                else:
                    # 没有视频，直接复制
                    temp_input_ids[i, :original_len_i] = input_ids[i]

            # 调用 get_rope_index
            final_position_ids, final_rope_deltas = self.get_rope_index(
                input_ids=temp_input_ids,
                image_grid_thw=image_grid_thw,
                video_grid_thw=video_grid_thw,
                video_metadata=video_metadata,
                attention_mask=final_attention_mask
            )
            
            # print(f"[DEBUG STEP5] Recalculated position_ids shape: {final_position_ids.shape}")
            # print(f"[DEBUG STEP5] Recalculated rope_deltas: {final_rope_deltas}")

        else:
            # 没有视频，使用原始 position_ids
            # print(f"[DEBUG STEP5] No video, using position_ids from Collator")
            final_position_ids = kwargs.get("position_ids")
            final_rope_deltas = kwargs.get("rope_deltas")
            
            if final_position_ids is None:
                final_position_ids, final_rope_deltas = self.get_rope_index(
                    input_ids=input_ids, 
                    attention_mask=attention_mask
                )

        # ========== Step 6: 调用 language_model ==========
        # print(f"===STEP 6 Calling Language Model===")
        # 确保 position_ids 最终形状正确 [3, B, L_new]
        if final_position_ids is None or final_position_ids.shape[2] != final_inputs_embeds.shape[1]:
             raise RuntimeError(f"Final position_ids shape {final_position_ids.shape if final_position_ids is not None else 'None'} "
                              f"mismatched with final inputs_embeds seq length {final_inputs_embeds.shape[1]}")

        # print(f"\n[DEBUG NAN] === Before language_model ===")
        # print(f"  final_inputs_embeds shape: {final_inputs_embeds.shape}, dtype: {final_inputs_embeds.dtype}")
        # print(f"  final_inputs_embeds has NaN: {torch.isnan(final_inputs_embeds).any()}")
        # print(f"  final_inputs_embeds has Inf: {torch.isinf(final_inputs_embeds).any()}")
        # Check value range (convert to float32 for stable min/max/mean)
        if not torch.isnan(final_inputs_embeds).any() and not torch.isinf(final_inputs_embeds).any():
             embeds_float = final_inputs_embeds.float()
             print(f"  final_inputs_embeds Range: min={embeds_float.min():.4f}, max={embeds_float.max():.4f}, mean={embeds_float.mean():.4f}")

        print(f"  final_position_ids shape: {final_position_ids.shape}, dtype: {final_position_ids.dtype}")
        # Position IDs are indices, usually don't cause NaN directly unless used incorrectly

        print(f"  final_attention_mask shape: {final_attention_mask.shape}, dtype: {final_attention_mask.dtype}")
        # Attention mask is usually 0/1, unlikely source of NaN

        outputs = self.language_model(
            input_ids=None,
            position_ids=final_position_ids,     # <--- 使用新计算的 position_ids
            attention_mask=final_attention_mask, # <--- 使用新计算的 attention_mask
            past_key_values=None,                # <--- 禁用缓存
            inputs_embeds=final_inputs_embeds,   # <--- 使用动态构建的 embeds
            cache_position=None,                 # <--- 禁用缓存
            **kwargs,
        )

        llm_output_hidden_state = outputs.last_hidden_state
        # print(f"\n[DEBUG NAN] === After language_model ===")
        # print(f"  LLM output shape: {llm_output_hidden_state.shape}, dtype: {llm_output_hidden_state.dtype}")
        # print(f"  LLM output has NaN: {torch.isnan(llm_output_hidden_state).any()}")
        # print(f"  LLM output has Inf: {torch.isinf(llm_output_hidden_state).any()}")
        # if not torch.isnan(llm_output_hidden_state).any() and not torch.isinf(llm_output_hidden_state).any():
        #      llm_output_float = llm_output_hidden_state.float()
        #      print(f"  LLM output Range: min={llm_output_float.min():.4f}, max={llm_output_float.max():.4f}, mean={llm_output_float.mean():.4f}")

        return Glm4vModelOutputWithPast(
            last_hidden_state=outputs.last_hidden_state,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            rope_deltas=final_rope_deltas, # <--- 使用新计算的 rope_deltas
        )

    def _expand_video_placeholders(
        self,
        input_ids: torch.LongTensor,
        attention_mask: Optional[torch.Tensor],
        video_metadata: Glm4vCubingVideoMetadata,
    ) -> Tuple[torch.LongTensor, Optional[torch.Tensor]]:
        """
        将 input_ids 中的单个 <|video|> token 扩展为 video_metadata.actual_num_tokens 个
        video_token_id，并相应扩展 attention_mask。进行 Padding。
        """
        batch_size, seq_len = input_ids.shape
        device = input_ids.device

        # 假设 video_metadata 对应整个 batch (或需要按 batch item 索引)
        # 这里简化为假设 batch size=1 或所有样本视频 token 数相同
        actual_tokens = video_metadata.actual_num_tokens
        video_token_id = self.config.video_token_id # 要重复插入的 ID
        pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else 0


        expanded_input_ids_list = []
        expanded_attention_mask_list = [] if attention_mask is not None else None

        for i in range(batch_size):
            video_positions = (input_ids[i] == video_token_id).nonzero(as_tuple=True)[0]

            if len(video_positions) == 0:
                expanded_input_ids_list.append(input_ids[i])
                if attention_mask is not None:
                    expanded_attention_mask_list.append(attention_mask[i])
                continue

            # 假设只有一个 video placeholder
            video_pos = video_positions[0].item()

            expanded_ids = torch.cat([
                input_ids[i, :video_pos],
                torch.full((actual_tokens,), video_token_id, dtype=input_ids.dtype, device=device),
                input_ids[i, video_pos+1:]
            ])
            expanded_input_ids_list.append(expanded_ids)

            if attention_mask is not None:
                expanded_mask = torch.cat([
                    attention_mask[i, :video_pos],
                    torch.ones(actual_tokens, dtype=attention_mask.dtype, device=device),
                    attention_mask[i, video_pos+1:]
                ])
                expanded_attention_mask_list.append(expanded_mask)

        # Padding
        max_len = max(ids.shape[0] for ids in expanded_input_ids_list)

        padded_input_ids = torch.stack([
            F.pad(ids, (0, max_len - ids.shape[0]), value=pad_token_id)
            for ids in expanded_input_ids_list
        ])

        padded_attention_mask = None
        if attention_mask is not None:
            padded_attention_mask = torch.stack([
                F.pad(mask, (0, max_len - mask.shape[0]), value=0) # Pad mask with 0
                for mask in expanded_attention_mask_list
            ])

        # print(f"[DEBUG EXPAND] Expanded input_ids shape: {padded_input_ids.shape}")
        # if padded_attention_mask is not None:
        #      print(f"[DEBUG EXPAND] Expanded attention_mask shape: {padded_attention_mask.shape}")

        return padded_input_ids, padded_attention_mask
        
@dataclass
@auto_docstring(
    custom_intro="""
    Base class for Glm4v causal language model (or autoregressive) outputs.
    """
)
class Glm4vCausalLMOutputWithPast(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Cache] = None
    hidden_states: Optional[tuple[torch.FloatTensor]] = None
    attentions: Optional[tuple[torch.FloatTensor]] = None
    rope_deltas: Optional[torch.LongTensor] = None


class Glm4vForConditionalGeneration(Glm4vPreTrainedModel, GenerationMixin):
    _checkpoint_conversion_mapping = {}
    _tied_weights_keys = ["lm_head.weight"]
    accepts_loss_kwargs = False

    def __init__(self, config):
        super().__init__(config)
        self.model = Glm4vModel(config)
        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)
        self.post_init()

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        """
        重写 from_pretrained 以自动初始化 Cubing 模块
        """
        # 调用父类的 from_pretrained
        model = super().from_pretrained(*args, **kwargs)
        
        # 自动初始化 Cubing 模块（如果需要）
        if hasattr(model, 'model') and hasattr(model.model, 'initialize_cubing_modules_if_needed'):
            model.model.initialize_cubing_modules_if_needed()
        
        return model
    

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def get_decoder(self):
        return self.model.get_decoder()

    def get_video_features(
        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None
    ):
        return self.model.get_video_features(pixel_values_videos, video_grid_thw)

    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):
        return self.model.get_image_features(pixel_values, image_grid_thw)

    @property
    def language_model(self):
        return self.model.language_model

    @property
    def visual(self):
        return self.model.visual

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        pixel_values: Optional[torch.Tensor] = None,
        pixel_values_videos: Optional[torch.FloatTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        videos_bound: Optional[list] = None,
        rope_deltas: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, Glm4vCausalLMOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
        
        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
            The temporal, height and width of feature shape of each image in LLM.
        
        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
            The temporal, height and width of feature shape of each video in LLM.
        
        videos_bound (`list`, *optional*):
            Frame boundaries for each video in the batch. Format: [(start_idx, end_idx), ...].
        
        rope_deltas (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            The rope index difference between sequence length and multimodal rope.

        """
        outputs = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            pixel_values_videos=pixel_values_videos,
            image_grid_thw=image_grid_thw,
            video_grid_thw=video_grid_thw,
            videos_bound=videos_bound,
            position_ids=position_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            **kwargs,
        )

        hidden_states = outputs[0]

        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])

        # print(f"\n[DEBUG NAN] === After lm_head (Logits) ===")
        # print(f"  logits shape: {logits.shape}, dtype: {logits.dtype}")
        # print(f"  logits has NaN: {torch.isnan(logits).any()}")
        # print(f"  logits has Inf: {torch.isinf(logits).any()}")
        if not torch.isnan(logits).any() and not torch.isinf(logits).any():
             logits_float = logits.float()
             print(f"  logits Range: min={logits_float.min():.4f}, max={logits_float.max():.4f}, mean={logits_float.mean():.4f}")

        loss = None
        if labels is not None:
            original_seq_len = labels.shape[1] # L_orig (e.g., 24)
            # print(f"[DEBUG LOSS]original_seq_len: {original_seq_len}")
            new_seq_len = logits.shape[1]      # L_new (e.g., 343)
            # print(f"[DEBUG LOSS]new_seq_len: {new_seq_len}")
            batch_size = labels.shape[0]
            device = labels.device
            ignore_index = -100 # Standard ignore index for cross entropy

            if original_seq_len != new_seq_len:
                # print(f"[DEBUG LOSS] Expanding labels from {original_seq_len} to {new_seq_len}")
                # We need to know where the video tokens were inserted.
                # This requires finding the video placeholder in the ORIGINAL input_ids.
                # Assuming input_ids (original) is available here, or passed through outputs
                # Let's assume original input_ids ARE available as function argument `input_ids`
                # (If not, this needs adjustment to get the original IDs)

                if input_ids is None:
                    raise RuntimeError("Original input_ids are required in Glm4vForConditionalGeneration.forward to expand labels correctly.")
                if input_ids.shape[1] != original_seq_len:
                    raise RuntimeError(f"Shape mismatch: labels length ({original_seq_len}) does not match original input_ids length ({input_ids.shape[1]})")


                video_placeholder_id = self.config.video_token_id
                num_tokens_to_insert = new_seq_len - original_seq_len + 1 # e.g., 343 - 24 + 1 = 320

                labels_expanded = torch.full((batch_size, new_seq_len), ignore_index, dtype=labels.dtype, device=device)

                for i in range(batch_size):
                    video_positions = (input_ids[i] == video_placeholder_id).nonzero(as_tuple=True)[0]
                    if len(video_positions) > 0:
                        video_pos_orig = video_positions[0].item() # Position in original sequence

                        # Copy labels before placeholder
                        labels_expanded[i, :video_pos_orig] = labels[i, :video_pos_orig]
                        # Video part is filled with ignore_index (already done by torch.full)
                        # Copy labels after placeholder
                        orig_after_start = video_pos_orig + 1
                        new_after_start = video_pos_orig + num_tokens_to_insert
                        len_after = original_seq_len - orig_after_start
                        if len_after > 0:
                                end_idx_new = min(new_after_start + len_after, new_seq_len)
                                end_idx_orig = min(orig_after_start + len_after, original_seq_len)
                                labels_expanded[i, new_after_start : end_idx_new] = labels[i, orig_after_start : end_idx_orig]
                    else: # No video placeholder found in original input_ids
                        # Copy original labels directly, padding the rest with ignore_index
                        labels_expanded[i, :original_seq_len] = labels[i]

                # Use the expanded labels for loss calculation
                labels_to_use = labels_expanded
                # print(f"[DEBUG LOSS] Using expanded labels shape: {labels_to_use.shape}")
                # print(f"\n[DEBUG NAN] === Before loss_function ===")
                # print(f"  logits shape (input to loss): {logits.shape}") # Should match above
                # print(f"  labels_to_use shape (input to loss): {labels_to_use.shape}")
                # print(f"  labels_to_use has values outside [-100, vocab_size-1]?: ",
                    # f"min={labels_to_use.min()}, max={labels_to_use.max()}, vocab_size={self.config.text_config.vocab_size}")

            else: # No expansion needed
                labels_to_use = labels
                print(f"[DEBUG LOSS] Using original labels shape: {labels_to_use.shape}")

            if labels_to_use is not None:
                # 假设 self.loss_function 处理 label shifting
                loss = self.loss_function(logits=logits, labels=labels_to_use, vocab_size=self.config.text_config.vocab_size)
                # print(f"[DEBUG NAN] === After loss_function ===")
                # print(f"  Calculated loss value: {loss.item() if loss is not None else 'None'}")
                # print(f"  Loss is NaN: {torch.isnan(loss).any() if loss is not None else 'N/A'}")
                # print(f"  Loss is Inf: {torch.isinf(loss).any() if loss is not None else 'N/A'}")
                print(f"[DEBUG LOSS] Calculated loss: {loss.item() if loss is not None else 'None'}")
            else:
                 print("[ERROR LOSS] labels_to_use is None, cannot calculate loss.")

            if self.config.use_cubing and self.training:
                aux_loss = self.model.get_cubing_aux_loss(alpha=0.001)
                loss = loss + aux_loss

        return Glm4vCausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            rope_deltas=outputs.rope_deltas,
        )
    
    def loss_function(self, logits, labels, vocab_size, ignore_index=-100):
        # Simplified version assuming standard cross entropy loss
        # Shift logits and labels for next token prediction
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        # Flatten the tokens
        shift_logits = shift_logits.view(-1, vocab_size)
        shift_labels = shift_labels.view(-1)
        # Enable model parallelism
        shift_labels = shift_labels.to(shift_logits.device)
        # Calculate loss
        loss = F.cross_entropy(shift_logits, shift_labels, ignore_index=ignore_index)
        return loss

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        use_cache=True,
        pixel_values=None,
        pixel_values_videos=None,
        image_grid_thw=None,
        video_grid_thw=None,
        **kwargs,
    ):
        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            position_ids=position_ids,
            pixel_values=pixel_values,
            pixel_values_videos=pixel_values_videos,
            image_grid_thw=image_grid_thw,
            video_grid_thw=video_grid_thw,
            use_cache=use_cache,
            **kwargs,
        )

        model_inputs["position_ids"] = None

        if cache_position[0] != 0:
            model_inputs["pixel_values"] = None
            model_inputs["pixel_values_videos"] = None

        return model_inputs

    def _get_image_nums_and_video_nums(
        self,
        input_ids: Optional[torch.LongTensor],
        inputs_embeds: Optional[torch.Tensor] = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        if inputs_embeds is not None:
            is_image = (
                inputs_embeds
                == self.get_input_embeddings()(
                    torch.tensor(self.config.image_start_token_id, dtype=torch.long, device=inputs_embeds.device)
                )
            )[..., 0]
            is_video_start = (
                inputs_embeds
                == self.get_input_embeddings()(
                    torch.tensor(self.config.video_start_token_id, dtype=torch.long, device=inputs_embeds.device)
                )
            )[..., 0]
            is_video_end = (
                inputs_embeds
                == self.get_input_embeddings()(
                    torch.tensor(self.config.video_end_token_id, dtype=torch.long, device=inputs_embeds.device)
                )
            )[..., 0]
        else:
            is_image = input_ids == self.config.image_start_token_id
            is_video_start = input_ids == self.config.video_start_token_id
            is_video_end = input_ids == self.config.video_end_token_id

        video_level = torch.cumsum(is_video_start.int() - is_video_end.int(), dim=1)
        inside_video = video_level > 0

        standalone_images = is_image & (~inside_video)

        image_counts = standalone_images.sum(dim=1)
        video_counts = is_video_start.sum(dim=1)

        return image_counts, video_counts

    def _expand_inputs_for_generation(
        self,
        expand_size: int = 1,
        is_encoder_decoder: bool = False,
        input_ids: Optional[torch.LongTensor] = None,
        **model_kwargs,
    ) -> tuple[torch.LongTensor, dict[str, Any]]:
        if expand_size == 1:
            return input_ids, model_kwargs

        visual_keys = ["pixel_values", "image_grid_thw", "pixel_values_videos", "video_grid_thw", "second_per_grid_ts"]

        def _expand_dict_for_generation_visual(dict_to_expand):
            image_grid_thw = model_kwargs.get("image_grid_thw", None)
            video_grid_thw = model_kwargs.get("video_grid_thw", None)
            image_nums, video_nums = self._get_image_nums_and_video_nums(
                input_ids, inputs_embeds=model_kwargs.get("inputs_embeds", None)
            )

            def _repeat_interleave_samples(x, lengths, repeat_times):
                samples = torch.split(x, lengths)
                repeat_args = [repeat_times] + [1] * (x.dim() - 1)
                result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)
                return result

            for key in dict_to_expand:
                if key == "pixel_values":
                    samples = torch.split(image_grid_thw, list(image_nums))
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "image_grid_thw":
                    lengths = list(image_nums)
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "pixel_values_videos":
                    samples = torch.split(video_grid_thw, list(video_nums))
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "video_grid_thw":
                    lengths = list(video_nums)
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "second_per_grid_ts":
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=list(video_nums), repeat_times=expand_size
                    )
            return dict_to_expand

        def _expand_dict_for_generation(dict_to_expand):
            for key in dict_to_expand:
                if (
                    key != "cache_position"
                    and dict_to_expand[key] is not None
                    and isinstance(dict_to_expand[key], torch.Tensor)
                    and key not in visual_keys
                ):
                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)
            return dict_to_expand

        model_kwargs = _expand_dict_for_generation_visual(model_kwargs)

        if input_ids is not None:
            input_ids = input_ids.repeat_interleave(expand_size, dim=0)

        model_kwargs = _expand_dict_for_generation(model_kwargs)

        if is_encoder_decoder:
            if model_kwargs.get("encoder_outputs") is None:
                raise ValueError("If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.")
            model_kwargs["encoder_outputs"] = _expand_dict_for_generation(model_kwargs["encoder_outputs"])

        return input_ids, model_kwargs

    def get_output_embeddings(self):
        """获取输出embedding层（lm_head）"""
        return self.lm_headß

    def set_output_embeddings(self, new_embeddings):
        """设置输出embedding层"""
        self.lm_head = new_embeddings

__all__ = ["Glm4vForConditionalGeneration", "Glm4vModel", "Glßm4vPreTrainedModel", "Glm4vTextModel", "Glm4vVisionModel"]
AutoModel.register(Glm4vConfig, Glm4vModel, exist_ok=True)
AutoModelForCausalLM.register(Glm4vConfig, Glm4vForConditionalGeneration, exist_ok=True)


# processing_glm4v_withCube.py
"""
Processor class for GLM4V with Cubing support.
"""

from typing import Any, List, Optional, Union

import torch
from transformers.image_utils import ImageInput
from transformers.video_utils import VideoInput

from transformers.processing_utils import ProcessorMixin
from transformers.tokenization_utils_base import PreTokenizedInput, TextInput
from transformers.utils import TensorType

from .configuration_glm4v_withCube import Glm4vConfig

from transformers import AutoProcessor


class Glm4vProcessor(ProcessorMixin):
    r"""
    Constructs a GLM4V processor which wraps a GLM4V image processor and a GLM4V tokenizer into a single processor.

    [`Glm4vProcessor`] offers all the functionalities of [`Glm4vCubingImageProcessor`] and [`PreTrainedTokenizer`].
    See the [`~Glm4vProcessor.__call__`] and [`~Glm4vProcessor.decode`] for more information.

    Args:
        image_processor ([`Glm4vCubingImageProcessor`]):
            The image processor is a required input.
        tokenizer ([`PreTrainedTokenizer`]):
            The tokenizer is a required input.
        config ([`Glm4vConfig`], *optional*):
            The model config for accessing Cubing parameters.
        auto_videos_bound (`bool`, *optional*, defaults to `True`):
            Whether to automatically construct videos_bound from video_grid_thw.
            Set to False if you want to manually provide videos_bound.
    """

    attributes = ["image_processor", "tokenizer"]
    image_processor_class = "Glm4vCubingImageProcessor"
    tokenizer_class = "AutoTokenizer"

    def __init__(
        self, 
        image_processor, 
        tokenizer, 
        config: Optional[Glm4vConfig] = None,
        auto_videos_bound: bool = True,
        **kwargs
    ):
        # 不调用 super().__init__，避免类型检查
        # super().__init__(image_processor, tokenizer)  
        
        self.image_processor = image_processor
        self.tokenizer = tokenizer
        
        # 设置自定义属性
        self.config = config
        self.auto_videos_bound = auto_videos_bound
        
        # 设置 _in_target_context_manager（ProcessorMixin 需要）
        self._in_target_context_manager = False

    def __call__(
        self,
        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,
        images: ImageInput = None,
        videos: VideoInput = None,
        return_videos_bound: bool = None,
        padding: Union[bool, str] = False,
        truncation: Union[bool, str] = None,
        max_length: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,
        **kwargs,
    ):
        """
        Main method to prepare for the model one or several sequences(s) and image(s)/video(s).

        Args:
            text (`str`, `List[str]`):
                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
                (pretokenized string).
            images (`PIL.Image.Image`, `List[PIL.Image.Image]`):
                The image or batch of images to be prepared. Each image should be a PIL image.
            videos (`List[PIL.Image.Image]`, `List[List[PIL.Image.Image]]`):
                The video or batch of videos to be prepared. Each video should be a list of PIL images (frames).
            return_videos_bound (`bool`, *optional*):
                Whether to return videos_bound in the output.
                If None, defaults to self.auto_videos_bound.
                Set to False to let the model auto-infer from video_grid_thw.
            padding (`bool`, `str`, *optional*, defaults to `False`):
                Padding strategy for tokenizer.
            truncation (`bool`, `str`, *optional*):
                Truncation strategy for tokenizer.
            max_length (`int`, *optional*):
                Maximum length for tokenizer.
            return_tensors (`str` or [`~utils.TensorType`], *optional*):
                Return tensor type.

        Returns:
            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:

            - **input_ids** -- List of token ids to be fed to a model.
            - **attention_mask** -- List of indices specifying which tokens should be attended to.
            - **pixel_values** -- Pixel values of images to be fed to a model.
            - **pixel_values_videos** -- Pixel values of videos to be fed to a model.
            - **image_grid_thw** -- Grid shape (temporal, height, width) for images.
            - **video_grid_thw** -- Grid shape (temporal, height, width) for videos.
            - **videos_bound** -- (Optional) Frame boundaries for videos.
        """
        # ========== Step 1: Process images (unchanged from native GLM4V) ==========
        image_inputs = {}
        if images is not None:
            image_inputs = self.image_processor(images=images, return_tensors=return_tensors)

        # ========== Step 2: Process videos (pixel-level processing unchanged) ==========
        video_inputs = {}
        videos_bound = None
        
        if videos is not None:
            # Process video pixels (same as native GLM4V)
            video_inputs = self.image_processor(images=videos, return_tensors=return_tensors)
            if 'image_grid_thw' in video_inputs:
                video_inputs['video_grid_thw'] = video_inputs.pop('image_grid_thw')
    
            if 'pixel_values' in video_inputs:
                video_inputs['pixel_values_videos'] = video_inputs.pop('pixel_values')
            
            # ✨ Calculate token counts based on mode
            if self.config is not None and self.config.use_cubing:
                # Cubing mode: calculate dynamic token count
                num_video_tokens, videos_bound = self._calculate_video_tokens_cubing(
                    video_inputs['video_grid_thw']
                )
            else:
                # Native mode: use original formula
                num_video_tokens = self._calculate_video_tokens_native(
                    video_inputs['video_grid_thw']
                )
                
                # Optionally construct videos_bound for auto-inference
                if self.auto_videos_bound:
                    _, videos_bound = self._construct_videos_bound(
                        video_inputs['video_grid_thw']
                    )
            
            # Store for later use in text processing
            video_inputs['_num_placeholder_tokens'] = num_video_tokens

        # ========== Step 3: Process text with correct placeholder counts ==========
        if text is not None:
            # Get placeholder token counts
            num_image_tokens = self._get_num_image_tokens(image_inputs)
            num_video_tokens = video_inputs.get('_num_placeholder_tokens', 0)
            
            # Replace <image>/<video> tags with correct number of image tokens
            if isinstance(text, str):
                text = [text]

            print(f"[DEBUG CALL] Retrieved num_video_tokens: {num_video_tokens}")
            if isinstance(text, str): text = [text] # Ensure list
            print(f"[DEBUG CALL] Text sample contains '<video>'? {'<video>' in text[0] if text else 'N/A'}")
            print(f"[DEBUG CALL] Text sample snippet: '{text[0][:200]}...'")
            
            processed_text = []
            for t in text:
                # This is a simplified placeholder - actual implementation 
                # would need to handle the specific token format of GLM4V
                t = self._insert_image_tokens(t, num_image_tokens, num_video_tokens)
                processed_text.append(t)
            
            # Tokenize
            text_inputs = self.tokenizer(
                processed_text,
                return_tensors=return_tensors,
                padding=padding,
                truncation=truncation,
                max_length=max_length,
                **kwargs,
            )
        else:
            text_inputs = {}

        # ========== Step 4: Combine all inputs ==========
        encoding = {**text_inputs, **image_inputs, **video_inputs}
        
        # Decide whether to return videos_bound
        if return_videos_bound is None:
            return_videos_bound = self.auto_videos_bound
        
        if videos_bound is not None and return_videos_bound:
            encoding['videos_bound'] = videos_bound
        
        # Clean up internal keys
        encoding.pop('_num_placeholder_tokens', None)
        
        return encoding

    def _calculate_video_tokens_native(self, video_grid_thw: torch.Tensor) -> int:
        """
        Calculate placeholder tokens for native GLM4V mode.
        
        Args:
            video_grid_thw: [num_videos, 3] - (temporal, height, width) for each video
        
        Returns:
            Total number of placeholder tokens
        """
        if self.config is None:
            # Fallback: use default spatial_merge_size=2
            spatial_merge_size = 2
        else:
            spatial_merge_size = self.config.vision_config.spatial_merge_size
        
        total_tokens = 0
        for t, h, w in video_grid_thw:
            # Original GLM4V formula
            tokens_per_video = (t.item() * h.item() * w.item()) // (spatial_merge_size ** 2)
            total_tokens += tokens_per_video
        
        return total_tokens

    def _calculate_video_tokens_cubing(self, video_grid_thw: torch.Tensor) -> tuple[int, list]:
        """
        Calculate placeholder tokens for Cubing mode.
        
        Args:
            video_grid_thw: [num_videos, 3] - (temporal, height, width) for each video
        
        Returns:
            tuple:
                - total_tokens: Total number of placeholder tokens
                - videos_bound: List of (start_frame, end_frame) tuples
        """
        if self.config is None:
            raise ValueError("Config is required for Cubing mode")
        
        total_tokens = 0
        videos_bound = []
        current_frame_idx = 0
        
        for t, h, w in video_grid_thw:
            num_frames = t.item()
            
            # Calculate number of cubes for this video
            # Formula from Cubing module: max(round(N_frames / fpq) - 1, 1)
            num_cubes = max(round(num_frames / self.config.cubing_fpq) - 1, 1)
            
            # Tokens per video = num_cubes * queries_per_cube
            tokens_per_video = num_cubes * self.config.resampler_num_queries
            
            # Add thumbnail token if enabled
            if self.config.cubing_use_thumbnail:
                tokens_per_video += 1
            
            total_tokens += tokens_per_video
            
            # Record frame boundaries for this video
            videos_bound.append((current_frame_idx, current_frame_idx + num_frames))
            current_frame_idx += num_frames
        
        return total_tokens, videos_bound

    def _construct_videos_bound(self, video_grid_thw: torch.Tensor) -> tuple[int, list]:
        """
        Construct videos_bound from video_grid_thw
        
        This is used when videos_bound is not provided but auto-inference is desired.
        Assumes each entry in video_grid_thw represents a separate video.
        
        Args:
            video_grid_thw: [num_videos, 3] - (temporal, height, width) for each video
        
        Returns:
            tuple:
                - num_videos: int - Number of videos
                - videos_bound: list - List of (start_frame, end_frame) tuples
        
        Example:
            >>> video_grid_thw = torch.tensor([[120, 24, 24], [80, 24, 24]])
            >>> num_videos, videos_bound = self._construct_videos_bound(video_grid_thw)
            >>> print(videos_bound)
            [(0, 120), (120, 200)]
        """
        videos_bound = []
        current_frame_idx = 0
        
        for t, h, w in video_grid_thw:
            num_frames = t.item()
            videos_bound.append((current_frame_idx, current_frame_idx + num_frames))
            current_frame_idx += num_frames
        
        return len(videos_bound), videos_bound

    def _get_num_image_tokens(self, image_inputs: dict) -> int:
        """
        Calculate number of placeholder tokens for images (unchanged from native).
        
        Args:
            image_inputs: Dictionary containing image_grid_thw
        
        Returns:
            Total number of image placeholder tokens
        """
        if not image_inputs or 'image_grid_thw' not in image_inputs:
            return 0
        
        image_grid_thw = image_inputs['image_grid_thw']
        
        if self.config is None:
            spatial_merge_size = 2
        else:
            spatial_merge_size = self.config.vision_config.spatial_merge_size
        
        total_tokens = 0
        for t, h, w in image_grid_thw:
            # Same formula as native GLM4V
            tokens_per_image = (t.item() * h.item() * w.item()) // (spatial_merge_size ** 2)
            total_tokens += tokens_per_image
        
        return total_tokens

    def _insert_image_tokens(
        self, 
        text: str, 
        num_image_tokens: int, 
        num_video_tokens: int
    ) -> str:
        """
        Replace <image> and <video> placeholders with actual image token IDs.
        
        NOTE: This is a simplified implementation. The actual GLM4V processor
        has more complex logic for handling special tokens and formatting.
        
        Args:
            text: Input text with <image> and <video> tags
            num_image_tokens: Number of tokens to insert for each <image>
            num_video_tokens: Number of tokens to insert for each <video>
        
        Returns:
            Text with placeholders replaced
        """
        # --- [Get image_token_string (unchanged)] ---
        if hasattr(self.tokenizer, 'image_token'):
            image_token_string = self.tokenizer.image_token
        else:
            image_token_string = "<image>" # Fallback

        # ===> Get the SPECIFIC string for the ID the MODEL expects <===
        specific_video_token_string = None
        expected_video_token_id = None
        if self.config and hasattr(self.config, 'video_token_id') and self.config.video_token_id is not None:
            expected_video_token_id = self.config.video_token_id # e.g., 151344
            try:
                # Convert the EXPECTED ID back to its string representation
                converted_token = self.tokenizer.convert_ids_to_tokens(expected_video_token_id)
                if isinstance(converted_token, list): converted_token = converted_token[0]
                # Use only if conversion is successful and not UNK
                if converted_token is not None and converted_token != self.tokenizer.unk_token:
                    specific_video_token_string = converted_token
                    print(f"[DEBUG INSERT] Found specific string '{specific_video_token_string}' for ID {expected_video_token_id}") # Debug Print
                else:
                     print(f"[WARNING INSERT] Tokenizer could not convert expected ID {expected_video_token_id} to a valid string (got {converted_token}). Falling back.")
                     specific_video_token_string = "<video>" # Fallback if ID conversion fails
            except Exception as e:
                 print(f"[WARNING INSERT] Error converting ID {expected_video_token_id}: {e}. Falling back.")
                 specific_video_token_string = "<video>" # General fallback on error
        else:
             print(f"[WARNING INSERT] Model config missing video_token_id. Falling back.")
             specific_video_token_string = "<video>" # Fallback if no config info

        if specific_video_token_string is None: # Should not happen with fallbacks, but safety check
            raise ValueError("Fatal error: Could not determine any video token string.")
        # ===> End Get Specific Video Token String <===

        # ===> Replace <video> tag with N repetitions of the SPECIFIC string <===
        if '<video>' in text and num_video_tokens > 0:
            # Repeat the specific video token string found above
            video_placeholder = specific_video_token_string * num_video_tokens
            # Replace only the first occurrence
            text = text.replace('<video>', video_placeholder, 1)
            print(f"[DEBUG INSERT] Replaced <video> with {num_video_tokens} instances of '{specific_video_token_string}'") # Debug Print
        # ===> End Video Replacement <===

        # --- [Replace <image> placeholders (unchanged)] ---
        if '<image>' in text and num_image_tokens > 0:
             if image_token_string is None:
                  raise ValueError("Could not determine the image token string for replacement.")
             image_placeholder = image_token_string * num_image_tokens
             text = text.replace('<image>', image_placeholder)

        return text

    def batch_decode(self, *args, **kwargs):
        """
        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`].
        Please refer to the docstring of this method for more information.
        """
        return self.tokenizer.batch_decode(*args, **kwargs)

    def decode(self, *args, **kwargs):
        """
        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`].
        Please refer to the docstring of this method for more information.
        """
        return self.tokenizer.decode(*args, **kwargs)

    @property
    def model_input_names(self):
        tokenizer_input_names = self.tokenizer.model_input_names
        image_processor_input_names = self.image_processor.model_input_names
        # videos_bound is optional, so it's included in the list
        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names + ["videos_bound"]))
    
__all__ = ["Glm4vProcessor"]
AutoProcessor.register(Glm4vConfig, Glm4vProcessor, exist_ok=True)


# coding=utf-8
# resampler_glm4v.py

"""
GLM4V 3D Resampler Module

Based on Quicksviewer's 3D Resampler, adapted for GLM4V architecture.
Compresses video cubes to fixed number of tokens while projecting to LLM dimension.
"""

import torch
import torch.nn as nn
from torch.nn.init import trunc_normal_


def get_3d_sincos_pos_embed(embed_dim, grid_size):
    """
    Generate 3D sinusoidal position embeddings
    
    Args:
        embed_dim: int - Embedding dimension
        grid_size: tuple - (T, H, W) grid size
    
    Returns:
        pos_embed: [T, H, W, embed_dim] - 3D position embeddings
    """
    grid_t, grid_h, grid_w = grid_size
    
    # Create coordinate grids
    grid_t = torch.arange(grid_t, dtype=torch.float32)
    grid_h = torch.arange(grid_h, dtype=torch.float32)
    grid_w = torch.arange(grid_w, dtype=torch.float32)
    
    # Create 3D meshgrid with correct order (T, H, W)
    grid = torch.meshgrid(grid_t, grid_h, grid_w, indexing='ij')
    grid = torch.stack(grid, dim=0)  # [3, T, H, W]
    
    pos_embed = get_3d_sincos_pos_embed_from_grid(embed_dim, grid)
    return pos_embed


def get_3d_sincos_pos_embed_from_grid(embed_dim, grid):
    """
    Generate 3D position embeddings from grid
    
    Dimension allocation: temporal 2/8, height 3/8, width 3/8
    
    Args:
        embed_dim: Embedding dimension (must be divisible by 8)
        grid: [3, T, H, W] - Coordinate grid
    
    Returns:
        emb: [T, H, W, embed_dim] - Position embeddings
    """
    assert embed_dim % 8 == 0
    
    # Temporal dimension: 2/8
    emb_t = get_1d_sincos_pos_embed_from_grid(embed_dim // 8 * 2, grid[0])
    # Height dimension: 3/8
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 8 * 3, grid[1])
    # Width dimension: 3/8
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 8 * 3, grid[2])
    
    emb = torch.cat([emb_t, emb_h, emb_w], dim=-1)  # [T, H, W, embed_dim]
    return emb


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    Generate 1D sinusoidal position embeddings
    
    Args:
        embed_dim: Embedding dimension
        pos: [T, H, W] - Position indices
    
    Returns:
        emb: [T, H, W, embed_dim] - Position embeddings
    """
    assert embed_dim % 2 == 0
    
    omega = torch.arange(embed_dim // 2, dtype=torch.float32)
    omega /= embed_dim / 2.
    omega = 1. / 10000 ** omega  # [embed_dim // 2]
    
    # Outer product: position × omega
    out = torch.einsum('thw,d->thwd', pos, omega)  # [T, H, W, embed_dim // 2]
    
    emb_sin = torch.sin(out)  # [T, H, W, embed_dim // 2]
    emb_cos = torch.cos(out)  # [T, H, W, embed_dim // 2]
    
    emb = torch.cat([emb_sin, emb_cos], dim=-1)  # [T, H, W, embed_dim]
    return emb


class Glm4vResampler(nn.Module):
    """
    GLM4V 3D Resampler
    
    A 3D perceiver-resampler network that compresses video cubes to fixed number
    of tokens while projecting from vision dimension to LLM dimension.
    
    Key features:
        - Uses learnable queries for compression
        - Applies 3D sinusoidal position embeddings
        - Single cross-attention layer
        - Projects from vision_dim (1536) to lm_dim (4096)
    """
    
    def __init__(self, config):
        super().__init__()
        
        # Configuration parameters
        self.vision_dim = config.vision_config.hidden_size
        self.lm_dim = config.text_config.hidden_size
        self.num_queries = config.resampler_num_queries
        self.num_heads = config.resampler_num_heads
        self.max_size = config.resampler_max_size
        
        # ===== 创建所有模块 =====
        self.query = nn.Parameter(torch.zeros(self.num_queries, self.lm_dim))
        self.kv_proj = nn.Linear(self.vision_dim, self.lm_dim, bias=False)
        self.attn = nn.MultiheadAttention(self.lm_dim, self.num_heads, batch_first=False)
        self.ln_q = nn.LayerNorm(self.lm_dim, eps=1e-6)
        self.ln_kv = nn.LayerNorm(self.lm_dim, eps=1e-6)
        self.ln_post = nn.LayerNorm(self.lm_dim, eps=1e-6)
        self.proj = nn.Parameter((self.lm_dim ** -0.5) * torch.randn(self.lm_dim, self.lm_dim))
        
        # Initialize 3D position embeddings cache
        self._set_3d_pos_cache(self.max_size)
        
        # ===== 初始化策略 =====
        # 1. 先用 apply 初始化所有 nn.Module
        self.apply(self._init_weights)
        
        # 2. 手动初始化 nn.Parameter（延迟到非 meta device）
        # 注意：这里不做任何操作，留给 _post_init_parameters
    
    def _init_weights(self, m):
        """Initialize nn.Module subclasses"""
        # 跳过 meta device
        if hasattr(m, 'weight') and m.weight.device.type == 'meta':
            return
        
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.MultiheadAttention):
            # ✅ 显式处理 MultiheadAttention
            self._init_multihead_attention(m)
    
    def _init_multihead_attention(self, m):
        """Initialize MultiheadAttention module"""
        if hasattr(m, 'in_proj_weight') and m.in_proj_weight is not None:
            if m.in_proj_weight.device.type != 'meta':
                nn.init.xavier_uniform_(m.in_proj_weight)
        
        if hasattr(m, 'in_proj_bias') and m.in_proj_bias is not None:
            if m.in_proj_bias.device.type != 'meta':
                nn.init.constant_(m.in_proj_bias, 0)
        
        # out_proj 是 nn.Linear，已经被 Linear 分支处理
    
    def _post_init_parameters(self):
        """
        Post-initialization for nn.Parameter
        应该在模型移到目标设备后调用
        """
        # 检查是否需要初始化
        if self.query.device.type == 'meta':
            return  # 还在 meta device，跳过
        
        # 初始化 query
        if torch.isnan(self.query).any() or self.query.std() < 1e-6:
            trunc_normal_(self.query, std=0.02)
            print(f"[POST-INIT] Initialized query")
        
        # 初始化 proj
        if torch.isnan(self.proj).any() or self.proj.std() < 1e-6:
            trunc_normal_(self.proj, std=0.02)
            print(f"[POST-INIT] Initialized proj")
        
        # 检查 MultiheadAttention（兜底）
        if hasattr(self.attn, 'in_proj_weight'):
            if self.attn.in_proj_weight.std() < 1e-6:
                nn.init.xavier_uniform_(self.attn.in_proj_weight)
                print(f"[POST-INIT] Initialized in_proj_weight")
        
        if hasattr(self.attn, 'in_proj_bias'):
            if torch.isnan(self.attn.in_proj_bias).any():
                nn.init.constant_(self.attn.in_proj_bias, 0)
                print(f"[POST-INIT] Initialized in_proj_bias")


    def _init_parameters(self):
        """初始化 nn.Parameter 类型的参数"""
        # 只在非 meta device 时初始化
        if self.query.device.type != 'meta':
            trunc_normal_(self.query, std=0.02)
            print(f"[INIT] query initialized on {self.query.device}")
        else:
            print(f"[INIT] query on meta device, skipping initialization")
        
        if self.proj.device.type != 'meta':
            trunc_normal_(self.proj, std=0.02)
            print(f"[INIT] proj initialized on {self.proj.device}")
        else:
            print(f"[INIT] proj on meta device, skipping initialization")

    def _set_3d_pos_cache(self, max_size, device='cpu'):
        """
        Initialize 3D position embeddings cache
        
        Args:
            max_size: tuple - (max_T, max_H, max_W)
            device: Device to create embeddings on
        """
        pos_embed = get_3d_sincos_pos_embed(self.lm_dim, max_size).to(device)
        self.register_buffer("pos_embed", pos_embed, persistent=False)
    
    def _adjust_pos_cache(self, tgt_size_range, device):
        """
        Dynamically adjust position embeddings cache if needed
        
        Args:
            tgt_size_range: list - [[t_start, t_end], [h_start, h_end], [w_start, w_end]]
            device: Target device
        """
        max_t = tgt_size_range[0][1]
        max_h = tgt_size_range[1][1]
        max_w = tgt_size_range[2][1]
        
        if max_t > self.max_size[0] or max_h > self.max_size[1] or max_w > self.max_size[2]:
            self.max_size = (
                max(max_t, self.max_size[0]),
                max(max_h, self.max_size[1]),
                max(max_w, self.max_size[2])
            )
            self._set_3d_pos_cache(self.max_size, device)
    
    # def _init_weights(self, m):
    #     """Initialize weights"""
    #     if isinstance(m, nn.Linear):
    #         trunc_normal_(m.weight, std=.02)
    #         if m.bias is not None:
    #             nn.init.constant_(m.bias, 0)
    #     elif isinstance(m, nn.LayerNorm):
    #         nn.init.constant_(m.bias, 0)
    #         nn.init.constant_(m.weight, 1.0)

    # def _init_weights(self, m):
    #     """Initialize weights"""
    #     # ✅ 跳过 meta device
    #     if hasattr(m, 'weight') and m.weight.device.type == 'meta':
    #         return
        
    #     if isinstance(m, nn.Linear):
    #         trunc_normal_(m.weight, std=.02)
    #         if m.bias is not None:
    #             nn.init.constant_(m.bias, 0)
    #     elif isinstance(m, nn.LayerNorm):
    #         nn.init.constant_(m.bias, 0)
    #         nn.init.constant_(m.weight, 1.0)
    #     elif isinstance(m, nn.MultiheadAttention):
    #         if hasattr(m, 'in_proj_weight') and m.in_proj_weight is not None:
    #             if m.in_proj_weight.device.type != 'meta':
    #                 nn.init.xavier_uniform_(m.in_proj_weight)
    #         if hasattr(m, 'in_proj_bias') and m.in_proj_bias is not None:
    #             if m.in_proj_bias.device.type != 'meta':
    #                 nn.init.constant_(m.in_proj_bias, 0)
    
    def forward(
        self,
        cube_features: torch.Tensor,
        tgt_size_range: list,
    ):
        """
        Compress cube features
        
        Args:
            cube_features: [N_i * 576, 1536] - Flattened cube patches
                - N_i: Number of frames in this cube
                - 576: Patches per frame (24×24)
                - 1536: Vision feature dimension
            
            tgt_size_range: list - 3D range specification
                - For images: [[0, h], [0, w]]
                - For cubes: [[t_start, t_end], [0, h], [0, w]]
        
        Returns:
            output: [num_queries, lm_dim] - Compressed tokens
                - If batch input, returns [B, num_queries, lm_dim]
        """
        # ========== Step 1: Normalize tgt_size_range ==========
        # Convert to 3D format
        tgt_size_range = [
            [0, _] if isinstance(_, int) else _
            for _ in tgt_size_range
        ]
        # print(f"[DEBUG RESAMPLER]step 1 tgt_size_range: {tgt_size_range}")
        # print(f"[DEBUG RESAMPLER] cube_features: {cube_features.shape}")
        # print(f"  cube features has NaN: {torch.isnan(cube_features).any()}")
        # print(f"  cube features has Inf: {torch.isinf(cube_features).any()}")

        if len(tgt_size_range) == 2:
            # Image: add temporal dimension
            tgt_size_range = [[0, 1], tgt_size_range[0], tgt_size_range[1]]
        
        # ========== Step 2: Reshape input ==========
        if cube_features.dim() == 3:
            # [B, L, D]
            pass
        else:
            # [L, D] → [1, L, D]
            cube_features = cube_features.unsqueeze(0)
        
        B, L, D = cube_features.shape
        device = cube_features.device
        dtype = cube_features.dtype
        # print(f"[DEBUG RESAMPLER]step 2 cube_features: {cube_features.shape}")
        # print(f"  cube features has NaN: {torch.isnan(cube_features).any()}")
        # print(f"  cube features has Inf: {torch.isinf(cube_features).any()}")
        
        # ========== Step 3: Calculate sizes ==========
        tgt_sizes = torch.tensor(
            [[r[1] - r[0] for r in tgt_size_range]],
            device=device,
            dtype=torch.int32
        ).repeat(B, 1)  # [B, 3]
        # print(f"[DEBUG RESAMPLER] tgt_sizes: {tgt_sizes}")
        
        patch_len = tgt_sizes[:, 0] * tgt_sizes[:, 1] * tgt_sizes[:, 2]
        max_patch_len = torch.max(patch_len)
        # print(f"[DEBUG RESAMPLER] patch_len: {patch_len}")
        # print(f"[DEBUG RESAMPLER] max_patch_len: {max_patch_len}")
        
        # ========== Step 4: KV projection ==========
        x = self.kv_proj(cube_features)  # [B, L, lm_dim]
        x = self.ln_kv(x).permute(1, 0, 2)  # [L, B, lm_dim]
        # print(f"kv_proj weight stats:")
        # print(f"  mean: {self.kv_proj.weight.mean()}")
        # print(f"  std: {self.kv_proj.weight.std()}")
        # print(f"  max: {self.kv_proj.weight.max()}")
        # print(f"  has NaN: {torch.isnan(self.kv_proj.weight).any()}")
        # print(f"  after KV proj has NaN: {torch.isnan(x).any()}") # false
        # print(f"  after KV proj has Inf: {torch.isinf(x).any()}")
        
        # ========== Step 5: Position embeddings ==========
        self._adjust_pos_cache(tgt_size_range, device)
        
        pos_embed_list = []
        key_padding_mask = torch.zeros(
            (B, max_patch_len), dtype=torch.bool, device=device
        )
        
        
        for i in range(B):
            t_range, h_range, w_range = tgt_size_range
            
            # Extract position embeddings for this sample
            pos = self.pos_embed[
                t_range[0]:t_range[1],  # Temporal
                h_range[0]:h_range[1],  # Height
                w_range[0]:w_range[1],  # Width
                :
            ]
            pos = pos.reshape(-1, self.lm_dim).to(dtype)
            pos_embed_list.append(pos)
            
            # Mark padding positions
            key_padding_mask[i, patch_len[i]:] = True
        
        # Pad position embeddings
        pos_embed = torch.nn.utils.rnn.pad_sequence(
            pos_embed_list, batch_first=True, padding_value=0.0
        ).permute(1, 0, 2)  # [L, B, lm_dim]
        # print(f"pos_embed has NaN: {torch.isnan(pos_embed).any()}")
        # print(f"pos_embed has Inf: {torch.isinf(pos_embed).any()}")
        
        # ========== Step 6: Cross-Attention ==========
        q = self.ln_q(self.query)  # [num_queries, lm_dim]
        q = q.unsqueeze(1).repeat(1, B, 1)  # [num_queries, B, lm_dim]
        # print(f"q weight stats:")
        # print(f"  query before layernorm: {torch.isnan(self.query).any()}")
        # print(f"  mean: {q.mean()}")
        # print(f"  std: {q.std()}")
        # print(f"  max: {q.max()}") 
        # print(f"  has NaN: {torch.isnan(q).any()}")
        # print(f"  after KV proj has NaN: {torch.isnan(x).any()}") # false
        # print(f"  after KV proj has Inf: {torch.isinf(x).any()}")
        
        # print(f"\n[DEBUG ATTN] Checking MultiheadAttention weights:")
        # for name, param in self.attn.named_parameters():
        #     has_nan = torch.isnan(param).any()
        #     mean = param.mean() if not has_nan else 'NaN'
        #     std = param.std() if not has_nan else 'NaN'
        #     print(f"  {name}: has_nan={has_nan}, mean={mean}, std={std}")

        # print(f"\n[DEBUG ATTN] Input stats:")
        # print(f"  q: shape={q.shape}, mean={q.mean():.6f}, std={q.std():.6f}, has_nan={torch.isnan(q).any()}")``
        # print(f"  k (x+pos): shape={(x+pos_embed).shape}, mean={(x+pos_embed).mean():.6f}, std={(x+pos_embed).std():.6f}, has_nan={torch.isnan(x+pos_embed).any()}")
        # print(f"  v (x): shape={x.shape}, mean={x.mean():.6f}, std={x.std():.6f}, has_nan={torch.isnan(x).any()}")
        # print(f"  key_padding_mask: shape={key_padding_mask.shape}, sum={key_padding_mask.sum()}")

        out = self.attn(
            q,  # [num_queries, B, lm_dim]
            x + pos_embed,  # [L, B, lm_dim] - KV with positional info
            x,  # [L, B, lm_dim] - Values without position
            key_padding_mask=key_padding_mask
        )[0]  # [num_queries, B, lm_dim]
        # print(f"out has NaN: {torch.isnan(out).any()}")
        # print(f"out has Inf: {torch.isinf(out).any()}")
        
        # ========== Step 7: Output projection ==========
        x = out.permute(1, 0, 2)  # [B, num_queries, lm_dim]
        x = self.ln_post(x)
        x = x @ self.proj  # [B, num_queries, lm_dim] 
        
        # If single sample, squeeze batch dimension
        if B == 1:
            x = x.squeeze(0)  # [num_queries, lm_dim]
        
        # print(f"resampler res shape: {x.shape}")
        # print(f"resampler res NaN: {torch.isnan(out).any()}")
        # print(f"resampler res Inf: {torch.isinf(out).any()}")
        return x
    
    @property
    def config(self):
        """Return configuration dict (for save/load)"""
        return {
            "vision_dim": self.vision_dim,
            "lm_dim": self.lm_dim,
            "num_queries": self.num_queries,
            "num_heads": self.num_heads,
            "max_size": self.max_size,
        }
    
__all__ = ["Glm4vResampler"]


# video_processing_glm4v.py
"""
GLM4V Video Processor with Cubing support
Handles video-specific preprocessing: batching, temporal alignment, etc.
"""

import torch
import torch.nn.functional as F
from typing import List, Optional, Union, Tuple, Dict
from PIL import Image

from transformers.image_processing_utils import BatchFeature
from transformers.image_utils import (
    ChannelDimension,
    PILImageResampling,
    to_channel_dimension_format,
)
from transformers.utils import TensorType

from .image_processing_glm4v import Glm4vCubingImageProcessor


def group_videos_by_shape(videos: List[torch.Tensor]) -> Tuple[Dict, List]:
    """
    按形状分组视频以便批处理
    
    Args:
        videos: List of [T, C, H, W] tensors
    
    Returns:
        grouped_videos: {shape: [video1, video2, ...]}
        grouped_indices: 原始顺序索引
    """
    grouped = {}
    indices = {}
    
    for idx, video in enumerate(videos):
        shape = tuple(video.shape)  # (T, C, H, W)
        if shape not in grouped:
            grouped[shape] = []
            indices[shape] = []
        grouped[shape].append(video)
        indices[shape].append(idx)
    
    return grouped, indices


def reorder_videos(grouped_videos: Dict, grouped_indices: Dict) -> List:
    """
    按原始顺序重组视频
    """
    flat_videos = []
    flat_indices = []
    
    for shape, videos in grouped_videos.items():
        flat_videos.extend(videos)
        flat_indices.extend(grouped_indices[shape])
    
    # 按原始索引排序
    sorted_pairs = sorted(zip(flat_indices, flat_videos))
    return [video for _, video in sorted_pairs]


class Glm4vVideoProcessor(Glm4vCubingImageProcessor):
    """
    Video-specific processor extending ImageProcessor
    
    Handles:
    - Batch processing of video frames
    - Temporal dimension alignment
    - Efficient memory usage
    """
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # 视频特定参数可以在这里添加
    
    def preprocess_videos(
        self,
        videos: Union[List[List[Image.Image]], List[torch.Tensor]],
        do_resize: Optional[bool] = None,
        size: Optional[dict] = None,
        resample: Optional[PILImageResampling] = None,
        do_rescale: Optional[bool] = None,
        rescale_factor: Optional[float] = None,
        do_normalize: Optional[bool] = None,
        image_mean: Optional[Union[float, List[float]]] = None,
        image_std: Optional[Union[float, List[float]]] = None,
        patch_size: Optional[int] = None,
        temporal_patch_size: Optional[int] = None,
        merge_size: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        **kwargs,
    ) -> BatchFeature:
        """
        批量处理视频
        
        Args:
            videos: List of videos, each video is:
                - List[PIL.Image]: List of frames
                - torch.Tensor: [T, C, H, W] tensor
        
        Returns:
            BatchFeature with:
                - pixel_values_videos: [total_patches, feature_dim]
                - video_grid_thw: [[t, h, w], ...]
        """
        # 参数默认值
        do_resize = do_resize if do_resize is not None else self.do_resize
        size = size if size is not None else self.size
        resample = resample if resample is not None else self.resample
        do_rescale = do_rescale if do_rescale is not None else self.do_rescale
        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor
        do_normalize = do_normalize if do_normalize is not None else self.do_normalize
        image_mean = image_mean if image_mean is not None else self.image_mean
        image_std = image_std if image_std is not None else self.image_std
        patch_size = patch_size if patch_size is not None else self.patch_size
        temporal_patch_size = temporal_patch_size if temporal_patch_size is not None else self.temporal_patch_size
        merge_size = merge_size if merge_size is not None else self.merge_size
        
        # 转换为torch.Tensor格式
        videos_tensors = []
        for video in videos:
            if isinstance(video, list):
                # List[PIL.Image] → torch.Tensor
                video_tensor = self._frames_to_tensor(video)
            else:
                video_tensor = video
            videos_tensors.append(video_tensor)
        
        # 🔑 按形状分组
        grouped_videos, grouped_indices = group_videos_by_shape(videos_tensors)
        
        processed_videos_grouped = {}
        processed_grids = {}
        
        # 🔑 批量处理每组
        for shape, video_batch in grouped_videos.items():
            processed, grids = self._process_video_batch(
                video_batch,
                do_resize=do_resize,
                size=size,
                resample=resample,
                do_rescale=do_rescale,
                rescale_factor=rescale_factor,
                do_normalize=do_normalize,
                image_mean=image_mean,
                image_std=image_std,
                patch_size=patch_size,
                temporal_patch_size=temporal_patch_size,
                merge_size=merge_size,
            )
            processed_videos_grouped[shape] = processed
            processed_grids[shape] = grids
        
        # 恢复原始顺序
        processed_videos = reorder_videos(processed_videos_grouped, grouped_indices)
        processed_grids = reorder_videos(processed_grids, grouped_indices)
        
        # 拼接所有视频
        pixel_values_videos = torch.cat(processed_videos, dim=0)
        video_grid_thw = torch.tensor(processed_grids, dtype=torch.long)
        
        data = {
            "pixel_values_videos": pixel_values_videos,
            "video_grid_thw": video_grid_thw,
        }
        
        return BatchFeature(data=data, tensor_type=return_tensors)
    
    def _frames_to_tensor(self, frames: List[Image.Image]) -> torch.Tensor:
        """
        将PIL图像列表转为tensor
        
        Args:
            frames: List of PIL.Image
        
        Returns:
            torch.Tensor: [T, C, H, W]
        """
        import numpy as np
        
        # 转为numpy数组
        frames_np = [np.array(frame) for frame in frames]
        frames_np = np.stack(frames_np)  # [T, H, W, C]
        
        # 转为torch并调整维度
        frames_tensor = torch.from_numpy(frames_np)
        frames_tensor = frames_tensor.permute(0, 3, 1, 2)  # [T, C, H, W]
        
        return frames_tensor.float()
    
    def _process_video_batch(
        self,
        video_batch: List[torch.Tensor],
        do_resize: bool,
        size: dict,
        resample: PILImageResampling,
        do_rescale: bool,
        rescale_factor: float,
        do_normalize: bool,
        image_mean: Union[float, List[float]],
        image_std: Union[float, List[float]],
        patch_size: int,
        temporal_patch_size: int,
        merge_size: int,
    ) -> Tuple[List[torch.Tensor], List[List[int]]]:
        """
        处理相同形状的视频批次
        
        Args:
            video_batch: List of [T, C, H, W] tensors (same shape)
        
        Returns:
            processed_videos: List of [N_patches, feature_dim]
            grids: List of [t, h, w]
        """
        # Stack成batch
        stacked = torch.stack(video_batch)  # [B, T, C, H, W]
        B, T, C, H, W = stacked.shape
        
        # 🔑 Resize (批量处理)
        if do_resize:
            from .image_processing_glm4v import smart_resize
            resized_h, resized_w = smart_resize(
                num_frames=T,
                height=H,
                width=W,
                temporal_factor=temporal_patch_size,
                factor=patch_size * merge_size,
                min_pixels=size["shortest_edge"],
                max_pixels=size["longest_edge"],
            )
            # 当作 B*T 张图片一次性resize
            stacked = stacked.view(B * T, C, H, W)
            stacked = self._resize_tensor(stacked, resized_h, resized_w, resample)
            stacked = stacked.view(B, T, C, resized_h, resized_w)
            H, W = resized_h, resized_w
        
        # 🔑 Rescale & Normalize (批量处理)
        if do_rescale:
            stacked = stacked * rescale_factor
        
        if do_normalize:
            mean = torch.tensor(image_mean).view(1, 1, -1, 1, 1)
            std = torch.tensor(image_std).view(1, 1, -1, 1, 1)
            stacked = (stacked - mean) / std
        
        # 🔑 Temporal对齐
        if T % temporal_patch_size != 0:
            pad_frames = temporal_patch_size - (T % temporal_patch_size)
            # 重复最后一帧
            last_frames = stacked[:, -1:].repeat(1, pad_frames, 1, 1, 1)
            stacked = torch.cat([stacked, last_frames], dim=1)
            T = stacked.shape[1]
        
        # 🔑 Reshape成patch格式
        grid_t = T // temporal_patch_size
        grid_h = H // patch_size
        grid_w = W // patch_size
        
        # Reshape: [B, T, C, H, W] → [B, grid_t, grid_h, grid_w, patch_features]
        patches = stacked.view(
            B,
            grid_t, temporal_patch_size,
            C,
            grid_h // merge_size, merge_size, patch_size,
            grid_w // merge_size, merge_size, patch_size,
        )
        # Permute: [B, grid_t, grid_h_merged, grid_w_merged, merge, merge, C, temp_patch, patch, patch]
        patches = patches.permute(0, 1, 4, 7, 5, 8, 3, 2, 6, 9)
        
        # Flatten: [B, grid_t*grid_h*grid_w, C*temp_patch*patch*patch]
        feature_dim = C * temporal_patch_size * patch_size * patch_size
        patches = patches.reshape(B, grid_t * grid_h * grid_w, feature_dim)
        
        # 拆分回List
        processed_videos = list(patches)
        grids = [[grid_t, grid_h, grid_w]] * B
        
        return processed_videos, grids
    
    def _resize_tensor(
        self, 
        tensor: torch.Tensor, 
        height: int, 
        width: int, 
        resample: PILImageResampling
    ) -> torch.Tensor:
        """
        Resize tensor using torch.nn.functional.interpolate
        
        Args:
            tensor: [N, C, H, W]
            height, width: target size
        
        Returns:
            Resized tensor [N, C, height, width]
        """
        mode_map = {
            PILImageResampling.BILINEAR: 'bilinear',
            PILImageResampling.BICUBIC: 'bicubic',
            PILImageResampling.NEAREST: 'nearest',
        }
        mode = mode_map.get(resample, 'bicubic')
        
        return F.interpolate(
            tensor,
            size=(height, width),
            mode=mode,
            align_corners=False if mode != 'nearest' else None
        )


__all__ = ["Glm4vVideoProcessor"]